{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7784c6aa8890d8f94e327df0e2046c06",
          "grade": false,
          "grade_id": "cell-772b7b8774cc890e",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_x58kwGo0wuy"
      },
      "source": [
        "# Assignment 4 - Average Reward Softmax Actor-Critic\n",
        "\n",
        "Welcome to your Course 3 Programming Assignment 4. In this assignment, you will implement **Average Reward Softmax Actor-Critic** in the Pendulum Swing-Up problem that you have seen earlier in the lecture. Through this assignment you will get hands-on experience in implementing actor-critic methods on a continuing task.\n",
        "\n",
        "**In this assignment, you will:**\n",
        "    1. Implement softmax actor-critic agent on a continuing task using the average reward formulation.\n",
        "    2. Understand how to parameterize the policy as a function to learn, in a discrete action environment.\n",
        "    3. Understand how to (approximately) sample the gradient of this objective to update the actor.\n",
        "    4. Understand how to update the critic using differential TD error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "49b8848ca28f7c932ea7a55ca5558a32",
          "grade": false,
          "grade_id": "cell-7c3b4d0ef54cf2bf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "iMtVSdVG0wu5"
      },
      "source": [
        "## Pendulum Swing-Up Environment\n",
        "\n",
        "In this assignment, we will be using a Pendulum environment, adapted from [Santamaría et al. (1998)](http://www.incompleteideas.net/papers/SSR-98.pdf). This is also the same environment that we used in the lecture. The diagram below illustrates the environment.\n",
        "\n",
        "<img src=\"data/pendulum_env.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
        "\n",
        "The environment consists of single pendulum that can swing 360 degrees. The pendulum is actuated by applying a torque on its pivot point. The goal is to get the pendulum to balance up-right from its resting position (hanging down at the bottom with no velocity) and maintain it as long as possible. The pendulum can move freely, subject only to gravity and the action applied by the agent.\n",
        "\n",
        "The state is 2-dimensional, which consists of the current angle $\\beta \\in [-\\pi, \\pi]$ (angle from the vertical upright position) and current angular velocity $\\dot{\\beta} \\in (-2\\pi, 2\\pi)$. The angular velocity is constrained in order to avoid damaging the pendulum system. If the angular velocity reaches this limit during simulation, the pendulum is reset to the resting position.\n",
        "The action is the angular acceleration, with discrete values $a \\in \\{-1, 0, 1\\}$ applied to the pendulum.\n",
        "For more details on environment dynamics you can refer to the original paper.\n",
        "\n",
        "The goal is to swing-up the pendulum and maintain its upright angle. Hence, the reward is the negative absolute angle from the vertical position: $R_{t} = -|\\beta_{t}|$\n",
        "\n",
        "Furthermore, since the goal is to reach and maintain a vertical position, there are no terminations nor episodes. Thus this problem can be formulated as a continuing task.\n",
        "\n",
        "Similar to the Mountain Car task, the action in this pendulum environment is not strong enough to move the pendulum directly to the desired position. The agent must learn to first move the pendulum away from its desired position and gain enough momentum to successfully swing-up the pendulum. And even after reaching the upright position the agent must learn to continually balance the pendulum in this unstable position."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "053cd6d5d174e9ad5e09566c5b77fb9a",
          "grade": false,
          "grade_id": "cell-77355b0e578b51a6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7yQrlJnG0wu6"
      },
      "source": [
        "## Packages\n",
        "\n",
        "You will use the following packages in this assignment.\n",
        "\n",
        "- [numpy](www.numpy.org) : Fundamental package for scientific computing with Python.\n",
        "- [matplotlib](http://matplotlib.org) : Library for plotting graphs in Python.\n",
        "- [RL-Glue](http://www.jmlr.org/papers/v10/tanner09a.html) : Library for reinforcement learning experiments.\n",
        "- [jdc](https://alexhagen.github.io/jdc/) : Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n",
        "- [tqdm](https://tqdm.github.io/) : A package to display progress bar when running experiments\n",
        "- plot_script : custom script to plot results\n",
        "- [tiles3](http://incompleteideas.net/tiles/tiles3.html) : A package that implements tile-coding.\n",
        "- pendulum_env : Pendulum Swing-up Environment\n",
        "\n",
        "**Please do not import other libraries** — this will break the autograder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8074478613373060bc01819879f799e1",
          "grade": false,
          "grade_id": "cell-67bdb1b8185106d1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "i_uoTjxy0wu8",
        "outputId": "9edb9e14-7a02-42a8-f571-2c75389df7b8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'rl_glue'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6ae3a3d0dbbc>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrl_glue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRLGlue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpendulum_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPendulumEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rl_glue'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Do not modify this cell!\n",
        "\n",
        "# Import necessary libraries\n",
        "# DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import itertools\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"Glues together an experiment, agent, and environment.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "class RLGlue:\n",
        "    \"\"\"RLGlue class\n",
        "\n",
        "    args:\n",
        "        env_name (string): the name of the module where the Environment class can be found\n",
        "        agent_name (string): the name of the module where the Agent class can be found\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_class, agent_class):\n",
        "        self.environment = env_class()\n",
        "        self.agent = agent_class()\n",
        "\n",
        "        self.total_reward = None\n",
        "        self.last_action = None\n",
        "        self.num_steps = None\n",
        "        self.num_episodes = None\n",
        "\n",
        "    def rl_init(self, agent_init_info={}, env_init_info={}):\n",
        "        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n",
        "        self.environment.env_init(env_init_info)\n",
        "        self.agent.agent_init(agent_init_info)\n",
        "\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 0\n",
        "        self.num_episodes = 0\n",
        "\n",
        "    def rl_start(self, agent_start_info={}, env_start_info={}):\n",
        "        \"\"\"Starts RLGlue experiment\n",
        "\n",
        "        Returns:\n",
        "            tuple: (state, action)\n",
        "        \"\"\"\n",
        "\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 1\n",
        "\n",
        "        last_state = self.environment.env_start()\n",
        "        self.last_action = self.agent.agent_start(last_state)\n",
        "\n",
        "        observation = (last_state, self.last_action)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def rl_agent_start(self, observation):\n",
        "        \"\"\"Starts the agent.\n",
        "\n",
        "        Args:\n",
        "            observation: The first observation from the environment\n",
        "\n",
        "        Returns:\n",
        "            The action taken by the agent.\n",
        "        \"\"\"\n",
        "        return self.agent.agent_start(observation)\n",
        "\n",
        "    def rl_agent_step(self, reward, observation):\n",
        "        \"\"\"Step taken by the agent\n",
        "\n",
        "        Args:\n",
        "            reward (float): the last reward the agent received for taking the\n",
        "                last action.\n",
        "            observation : the state observation the agent receives from the\n",
        "                environment.\n",
        "\n",
        "        Returns:\n",
        "            The action taken by the agent.\n",
        "        \"\"\"\n",
        "        return self.agent.agent_step(reward, observation)\n",
        "\n",
        "    def rl_agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates\n",
        "\n",
        "        Args:\n",
        "            reward (float): the reward the agent received when terminating\n",
        "        \"\"\"\n",
        "        self.agent.agent_end(reward)\n",
        "\n",
        "    def rl_env_start(self):\n",
        "        \"\"\"Starts RL-Glue environment.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): reward, state observation, boolean\n",
        "                indicating termination\n",
        "        \"\"\"\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 1\n",
        "\n",
        "        this_observation = self.environment.env_start()\n",
        "\n",
        "        return this_observation\n",
        "\n",
        "    def rl_env_step(self, action):\n",
        "        \"\"\"Step taken by the environment based on action from agent\n",
        "\n",
        "        Args:\n",
        "            action: Action taken by agent.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): reward, state observation, boolean\n",
        "                indicating termination.\n",
        "        \"\"\"\n",
        "        ro = self.environment.env_step(action)\n",
        "        (this_reward, _, terminal) = ro\n",
        "\n",
        "        self.total_reward += this_reward\n",
        "\n",
        "        if terminal:\n",
        "            self.num_episodes += 1\n",
        "        else:\n",
        "            self.num_steps += 1\n",
        "\n",
        "        return ro\n",
        "\n",
        "    def rl_step(self):\n",
        "        \"\"\"Step taken by RLGlue, takes environment step and either step or\n",
        "            end by agent.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, action, Boolean): reward, last state observation,\n",
        "                last action, boolean indicating termination\n",
        "        \"\"\"\n",
        "\n",
        "        (reward, last_state, term) = self.environment.env_step(self.last_action)\n",
        "\n",
        "        self.total_reward += reward;\n",
        "\n",
        "        if term:\n",
        "            self.num_episodes += 1\n",
        "            self.agent.agent_end(reward)\n",
        "            roat = (reward, last_state, None, term)\n",
        "        else:\n",
        "            self.num_steps += 1\n",
        "            self.last_action = self.agent.agent_step(reward, last_state)\n",
        "            roat = (reward, last_state, self.last_action, term)\n",
        "\n",
        "        return roat\n",
        "\n",
        "    def rl_cleanup(self):\n",
        "        \"\"\"Cleanup done at end of experiment.\"\"\"\n",
        "        self.environment.env_cleanup()\n",
        "        self.agent.agent_cleanup()\n",
        "\n",
        "    def rl_agent_message(self, message):\n",
        "        \"\"\"Message passed to communicate with agent during experiment\n",
        "\n",
        "        Args:\n",
        "            message: the message (or question) to send to the agent\n",
        "\n",
        "        Returns:\n",
        "            The message back (or answer) from the agent\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return self.agent.agent_message(message)\n",
        "\n",
        "    def rl_env_message(self, message):\n",
        "        \"\"\"Message passed to communicate with environment during experiment\n",
        "\n",
        "        Args:\n",
        "            message: the message (or question) to send to the environment\n",
        "\n",
        "        Returns:\n",
        "            The message back (or answer) from the environment\n",
        "\n",
        "        \"\"\"\n",
        "        return self.environment.env_message(message)\n",
        "\n",
        "    def rl_episode(self, max_steps_this_episode):\n",
        "        \"\"\"Runs an RLGlue episode\n",
        "\n",
        "        Args:\n",
        "            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n",
        "\n",
        "        Returns:\n",
        "            Boolean: if the episode should terminate\n",
        "        \"\"\"\n",
        "        is_terminal = False\n",
        "\n",
        "        self.rl_start()\n",
        "\n",
        "        while (not is_terminal) and ((max_steps_this_episode == 0) or\n",
        "                                     (self.num_steps < max_steps_this_episode)):\n",
        "            rl_step_result = self.rl_step()\n",
        "            is_terminal = rl_step_result[3]\n",
        "\n",
        "        return is_terminal\n",
        "\n",
        "    def rl_return(self):\n",
        "        \"\"\"The total reward\n",
        "\n",
        "        Returns:\n",
        "            float: the total reward\n",
        "        \"\"\"\n",
        "        return self.total_reward\n",
        "\n",
        "    def rl_num_steps(self):\n",
        "        \"\"\"The total number of steps taken\n",
        "\n",
        "        Returns:\n",
        "            Int: the total number of steps taken\n",
        "        \"\"\"\n",
        "        return self.num_steps\n",
        "\n",
        "    def rl_num_episodes(self):\n",
        "        \"\"\"The number of episodes\n",
        "\n",
        "        Returns\n",
        "            Int: the total number of episodes\n",
        "\n",
        "        \"\"\"\n",
        "        return self.num_episodes\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Vh4S9N4A0470"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"Abstract environment base class for RL-Glue-py.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "\n",
        "class BaseEnvironment:\n",
        "    \"\"\"Implements the environment for an RLGlue environment\n",
        "\n",
        "    Note:\n",
        "        env_init, env_start, env_step, env_cleanup, and env_message are required\n",
        "        methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        reward = None\n",
        "        observation = None\n",
        "        termination = None\n",
        "        self.reward_obs_term = (reward, observation, termination)\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"Setup for the environment called when the experiment first starts.\n",
        "\n",
        "        Note:\n",
        "            Initialize a tuple with the reward, first state observation, boolean\n",
        "            indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_start(self):\n",
        "        \"\"\"The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_cleanup(self):\n",
        "        \"\"\"Cleanup done after the environment ends\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_message(self, message):\n",
        "        \"\"\"A message asking the environment for information\n",
        "\n",
        "        Args:\n",
        "            message: the message passed to the environment\n",
        "\n",
        "        Returns:\n",
        "            the response (or answer) to the message\n",
        "        \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class PendulumEnvironment(BaseEnvironment):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rand_generator = None\n",
        "        self.ang_velocity_range = None\n",
        "        self.dt = None\n",
        "        self.viewer = None\n",
        "        self.gravity = None\n",
        "        self.mass = None\n",
        "        self.length = None\n",
        "\n",
        "        self.valid_actions = None\n",
        "        self.actions = None\n",
        "\n",
        "\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"\n",
        "        Setup for the environment called when the experiment first starts.\n",
        "\n",
        "        Set parameters needed to setup the pendulum swing-up environment.\n",
        "        \"\"\"\n",
        "        # set random seed for each run\n",
        "        self.rand_generator = np.random.RandomState(env_info.get(\"seed\"))\n",
        "\n",
        "        self.ang_velocity_range = [-2 * np.pi, 2 * np.pi]\n",
        "        self.dt = 0.05\n",
        "        self.viewer = None\n",
        "        self.gravity = 9.8\n",
        "        self.mass = float(1./3.)\n",
        "        self.length = float(3./2.)\n",
        "\n",
        "        self.valid_actions = (0,1,2)\n",
        "        self.actions = [-1,0,1]\n",
        "\n",
        "        self.last_action = None\n",
        "\n",
        "    def env_start(self):\n",
        "        \"\"\"\n",
        "        The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "        ### set self.reward_obs_term tuple accordingly (3~5 lines)\n",
        "        # Angle starts at -pi or pi, and Angular velocity at 0.\n",
        "        # reward = ?\n",
        "        # observation = ?\n",
        "        # is_terminal = ?\n",
        "\n",
        "        beta = -np.pi\n",
        "        betadot = 0.\n",
        "\n",
        "        reward = 0.0\n",
        "        observation = np.array([beta, betadot])\n",
        "        is_terminal = False\n",
        "\n",
        "        self.reward_obs_term = (reward, observation, is_terminal)\n",
        "\n",
        "        # return first state observation from the environment\n",
        "        return self.reward_obs_term[1]\n",
        "\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "        ### set reward, observation, and is_terminal correctly (10~12 lines)\n",
        "        # Update the state according to the transition dynamics\n",
        "        # Remember to normalize the angle so that it is always between -pi and pi.\n",
        "        # If the angular velocity exceeds the bound, reset the state to the resting position\n",
        "        # Compute reward according to the new state, and is_terminal should always be False\n",
        "        #\n",
        "        # reward = ?\n",
        "        # observation = ?\n",
        "        # is_terminal = ?\n",
        "\n",
        "        # Check if action is valid\n",
        "        assert(action in self.valid_actions)\n",
        "\n",
        "        last_state = self.reward_obs_term[1]\n",
        "        last_beta, last_betadot = last_state\n",
        "        self.last_action = action\n",
        "\n",
        "        betadot = last_betadot + 0.75 * (self.actions[action] + self.mass * self.length * self.gravity * np.sin(last_beta)) / (self.mass * self.length**2) * self.dt\n",
        "\n",
        "        beta = last_beta + betadot * self.dt\n",
        "\n",
        "        # normalize angle\n",
        "        beta = ((beta + np.pi) % (2*np.pi)) - np.pi\n",
        "\n",
        "        # reset if out of bound\n",
        "        if betadot < self.ang_velocity_range[0] or betadot > self.ang_velocity_range[1]:\n",
        "            beta = -np.pi\n",
        "            betadot = 0.\n",
        "\n",
        "        # compute reward\n",
        "        reward = -(np.abs(((beta+np.pi) % (2 * np.pi)) - np.pi))\n",
        "        observation = np.array([beta, betadot])\n",
        "        is_terminal = False\n",
        "\n",
        "\n",
        "        self.reward_obs_term = (reward, observation, is_terminal)\n",
        "\n",
        "        return self.reward_obs_term"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9JcqodL91ASQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"An abstract class that specifies the Agent API for RL-Glue-py.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "\n",
        "class BaseAgent:\n",
        "    \"\"\"Implements the agent for an RL-Glue environment.\n",
        "    Note:\n",
        "        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n",
        "        agent_message are required methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_init(self, agent_info= {}):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_start(self, observation):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            observation (Numpy array): the state observation from the environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            observation (Numpy array): the state observation from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the terminal state.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_cleanup(self):\n",
        "        \"\"\"Cleanup done after the agent ends.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_message(self, message):\n",
        "        \"\"\"A function used to pass information from the agent to the experiment.\n",
        "        Args:\n",
        "            message: The message passed to the agent.\n",
        "        Returns:\n",
        "            The response (or answer) to the message.\n",
        "        \"\"\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nsw__otf1LwH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot result\n",
        "def plot_result(agent_parameters, directory):\n",
        "\n",
        "    plt1_agent_sweeps = []\n",
        "    plt2_agent_sweeps = []\n",
        "\n",
        "    x_range = 20000\n",
        "    plt_xticks = [0, 4999, 9999, 14999, 19999]\n",
        "    plt_xlabels = [1, 5000, 10000, 15000, 20000]\n",
        "    plt1_yticks = range(0, -6001, -2000)\n",
        "    plt2_yticks = range(-3, 1, 1)\n",
        "\n",
        "\n",
        "    # single plots: Exp Avg reward\n",
        "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12,14))\n",
        "\n",
        "    for num_tilings in agent_parameters[\"num_tilings\"]:\n",
        "        for num_tiles in agent_parameters[\"num_tiles\"]:\n",
        "            for actor_ss in agent_parameters[\"actor_step_size\"]:\n",
        "                for critic_ss in agent_parameters[\"critic_step_size\"]:\n",
        "                    for avg_reward_ss in agent_parameters[\"avg_reward_step_size\"]:\n",
        "\n",
        "                        load_name = 'ActorCriticSoftmax_tilings_{}_tiledim_{}_actor_ss_{}_critic_ss_{}_avg_reward_ss_{}'.format(num_tilings, num_tiles, actor_ss, critic_ss, avg_reward_ss)\n",
        "\n",
        "                        ### plot1\n",
        "                        file_type1 = \"total_return\"\n",
        "                        data = np.load('{}/{}_{}.npy'.format(directory, load_name, file_type1))\n",
        "\n",
        "                        data_mean = np.mean(data, axis=0)\n",
        "                        data_std_err = np.std(data, axis=0)/np.sqrt(len(data))\n",
        "\n",
        "                        data_mean = data_mean[:x_range]\n",
        "                        data_std_err = data_std_err[:x_range]\n",
        "\n",
        "                        plt_x_legend = range(0,len(data_mean))[:x_range]\n",
        "\n",
        "                        ax[0].fill_between(plt_x_legend, data_mean - data_std_err, data_mean + data_std_err, alpha = 0.2)\n",
        "                        graph_current_data, = ax[0].plot(plt_x_legend, data_mean, linewidth=1.0, label=\"actor_ss: {}/32, critic_ss: {}/32, avg reward step_size: {}\".format(actor_ss, critic_ss, avg_reward_ss))\n",
        "                        plt1_agent_sweeps.append(graph_current_data)\n",
        "\n",
        "\n",
        "                        ### plot2\n",
        "                        file_type2 = \"exp_avg_reward\"\n",
        "                        data = np.load('{}/{}_{}.npy'.format(directory, load_name, file_type2))\n",
        "\n",
        "                        data_mean = np.mean(data, axis=0)\n",
        "                        data_std_err = np.std(data, axis=0)/np.sqrt(len(data))\n",
        "\n",
        "                        data_mean = data_mean[:x_range]\n",
        "                        data_std_err = data_std_err[:x_range]\n",
        "\n",
        "                        plt_x_legend = range(1,len(data_mean) + 1)[:x_range]\n",
        "\n",
        "                        ax[1].fill_between(plt_x_legend, data_mean - data_std_err, data_mean + data_std_err, alpha = 0.2)\n",
        "                        graph_current_data, = ax[1].plot(plt_x_legend, data_mean, linewidth=1.0, label=\"actor: {}/32, critic: {}/32, avg reward: {}\".format(actor_ss, critic_ss, avg_reward_ss))\n",
        "                        plt2_agent_sweeps.append(graph_current_data)\n",
        "\n",
        "    # plot 1\n",
        "    ax[0].legend(handles=[*plt1_agent_sweeps])\n",
        "    ax[0].set_xticks(plt_xticks)\n",
        "    ax[0].set_yticks(plt1_yticks)\n",
        "    ax[0].set_xticklabels(plt_xlabels)\n",
        "    ax[0].set_yticklabels(plt1_yticks)\n",
        "\n",
        "    ax[0].set_title(\"Return per Step\")\n",
        "    ax[0].set_xlabel('Training steps')\n",
        "    ax[0].set_ylabel('Total Return', rotation=90)\n",
        "    ax[0].set_xlim([0,20000])\n",
        "\n",
        "    # plot 2\n",
        "    ax[1].legend(handles=[*plt2_agent_sweeps])\n",
        "    ax[1].set_xticks(plt_xticks)\n",
        "    ax[1].set_yticks(plt2_yticks)\n",
        "\n",
        "    ax[1].set_title(\"Exponential Average Reward per Step\")\n",
        "    ax[1].set_xlabel('Training steps')\n",
        "    ax[1].set_ylabel('Exponential Average Reward', rotation=90)\n",
        "    ax[1].set_xticklabels(plt_xlabels)\n",
        "    ax[1].set_yticklabels(plt2_yticks)\n",
        "    ax[1].set_xlim([0,20000])\n",
        "    ax[1].set_ylim([-3, 0.16])\n",
        "\n",
        "    plt.suptitle(\"Average Reward Softmax Actor-Critic ({} Runs)\".format(len(data)),fontsize=16, fontweight='bold', y=1.03)\n",
        "\n",
        "    # ax[1].legend(handles=plt2_agent_sweeps)\n",
        "\n",
        "    # ax[1].set_title(\"Softmax policy Actor-Critic: Average Reward per Step ({} Runs)\".format(len(avg_reward)))\n",
        "    # ax[1].set_xlabel('Training steps')\n",
        "    # ax[1].set_ylabel('Average Reward', rotation=0, labelpad=40)\n",
        "    # ax[1].set_xticklabels(plt_xticks)\n",
        "    # ax[1].set_yticklabels(plt_yticks)\n",
        "    # ax.axhline(y=0.1, linestyle='dashed', linewidth=1.0, color='black')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # plt.suptitle(\"{}-State Aggregation\".format(num_agg_states),fontsize=16, fontweight='bold', y=1.03)\n",
        "    # plt.suptitle(\"Average Reward ActorCritic\",fontsize=16, fontweight='bold', y=1.03)\n",
        "    plt.show()\n",
        "\n",
        "def plot_sweep_result(directory):\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,7))\n",
        "\n",
        "    plt_agent_sweeps = []\n",
        "\n",
        "    x_range = 20000\n",
        "    plt_xticks = [0, 4999, 9999, 14999, 19999]\n",
        "    plt_xlabels = [1, 5000, 10000, 15000, 20000]\n",
        "    plt2_yticks = range(-3, 1, 1)\n",
        "\n",
        "    top_results = [{\"actor_ss\": 0.25, \"critic_ss\": 2, \"avg_reward_ss\": 0.03125},\n",
        "                  {\"actor_ss\": 0.25, \"critic_ss\": 2, \"avg_reward_ss\": 0.015625},\n",
        "                  {\"actor_ss\": 0.5, \"critic_ss\": 2, \"avg_reward_ss\": 0.0625},\n",
        "                  {\"actor_ss\": 1, \"critic_ss\": 2, \"avg_reward_ss\": 0.0625},\n",
        "                  {\"actor_ss\": 0.25, \"critic_ss\": 1, \"avg_reward_ss\": 0.015625}]\n",
        "\n",
        "    for setting in top_results:\n",
        "\n",
        "        num_tilings = 32\n",
        "        num_tiles = 8\n",
        "        actor_ss = setting[\"actor_ss\"]\n",
        "        critic_ss = setting[\"critic_ss\"]\n",
        "        avg_reward_ss = setting[\"avg_reward_ss\"]\n",
        "\n",
        "        load_name = 'ActorCriticSoftmax_tilings_{}_tiledim_{}_actor_ss_{}_critic_ss_{}_avg_reward_ss_{}'.format(num_tilings, num_tiles, actor_ss, critic_ss, avg_reward_ss)\n",
        "\n",
        "        file_type2 = \"exp_avg_reward\"\n",
        "        data = np.load('{}/{}_{}.npy'.format(directory, load_name, file_type2))\n",
        "\n",
        "        data_mean = np.mean(data, axis=0)\n",
        "        data_std_err = np.std(data, axis=0)/np.sqrt(len(data))\n",
        "\n",
        "        data_mean = data_mean[:x_range]\n",
        "        data_std_err = data_std_err[:x_range]\n",
        "\n",
        "        plt_x_legend = range(1,len(data_mean) + 1)[:x_range]\n",
        "\n",
        "        ax.fill_between(plt_x_legend, data_mean - data_std_err, data_mean + data_std_err, alpha = 0.2)\n",
        "        graph_current_data, = ax.plot(plt_x_legend, data_mean, linewidth=1.0, label=\"actor: {}/32, critic: {}/32, avg reward: {}\".format(actor_ss, critic_ss, avg_reward_ss))\n",
        "        plt_agent_sweeps.append(graph_current_data)\n",
        "\n",
        "    ax.legend(handles=[*plt_agent_sweeps])\n",
        "    ax.set_xticks(plt_xticks)\n",
        "    ax.set_yticks(plt2_yticks)\n",
        "\n",
        "    ax.set_title(\"Exponential Average Reward per Step ({} Runs)\".format(len(data)))\n",
        "    ax.set_xlabel('Training steps')\n",
        "    ax.set_ylabel('Exponential Average Reward', rotation=90)\n",
        "    ax.set_xticklabels(plt_xlabels)\n",
        "    ax.set_yticklabels(plt2_yticks)\n",
        "    ax.set_xlim([0, 20000])\n",
        "    ax.set_ylim([-3.5, 0.16])\n",
        ""
      ],
      "metadata": {
        "cellView": "form",
        "id": "4Ne8hDDH1RzN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\"\"\"\n",
        "Tile Coding Software version 3.0beta\n",
        "by Rich Sutton\n",
        "based on a program created by Steph Schaeffer and others\n",
        "External documentation and recommendations on the use of this code is available in the\n",
        "reinforcement learning textbook by Sutton and Barto, and on the web.\n",
        "These need to be understood before this code is.\n",
        "\n",
        "This software is for Python 3 or more.\n",
        "\n",
        "This is an implementation of grid-style tile codings, based originally on\n",
        "the UNH CMAC code (see http://www.ece.unh.edu/robots/cmac.htm), but by now highly changed.\n",
        "Here we provide a function, \"tiles\", that maps floating and integer\n",
        "variables to a list of tiles, and a second function \"tiles-wrap\" that does the same while\n",
        "wrapping some floats to provided widths (the lower wrap value is always 0).\n",
        "\n",
        "The float variables will be gridded at unit intervals, so generalization\n",
        "will be by approximately 1 in each direction, and any scaling will have\n",
        "to be done externally before calling tiles.\n",
        "\n",
        "Num-tilings should be a power of 2, e.g., 16. To make the offsetting work properly, it should\n",
        "also be greater than or equal to four times the number of floats.\n",
        "\n",
        "The first argument is either an index hash table of a given size (created by (make-iht size)),\n",
        "an integer \"size\" (range of the indices from 0), or nil (for testing, indicating that the tile\n",
        "coordinates are to be returned without being converted to indices).\n",
        "\"\"\"\n",
        "\n",
        "basehash = hash\n",
        "\n",
        "class IHT:\n",
        "    \"Structure to handle collisions\"\n",
        "    def __init__(self, sizeval):\n",
        "        self.size = sizeval\n",
        "        self.overfullCount = 0\n",
        "        self.dictionary = {}\n",
        "\n",
        "    def __str__(self):\n",
        "        \"Prepares a string for printing whenever this object is printed\"\n",
        "        return \"Collision table:\" + \\\n",
        "               \" size:\" + str(self.size) + \\\n",
        "               \" overfullCount:\" + str(self.overfullCount) + \\\n",
        "               \" dictionary:\" + str(len(self.dictionary)) + \" items\"\n",
        "\n",
        "    def count (self):\n",
        "        return len(self.dictionary)\n",
        "\n",
        "    def fullp (self):\n",
        "        return len(self.dictionary) >= self.size\n",
        "\n",
        "    def getindex (self, obj, readonly=False):\n",
        "        d = self.dictionary\n",
        "        if obj in d: return d[obj]\n",
        "        elif readonly: return None\n",
        "        size = self.size\n",
        "        count = self.count()\n",
        "        if count >= size:\n",
        "            if self.overfullCount==0: print('IHT full, starting to allow collisions')\n",
        "            self.overfullCount += 1\n",
        "            return basehash(obj) % self.size\n",
        "        else:\n",
        "            d[obj] = count\n",
        "            return count\n",
        "\n",
        "def hashcoords(coordinates, m, readonly=False):\n",
        "    if type(m)==IHT: return m.getindex(tuple(coordinates), readonly)\n",
        "    if type(m)==int: return basehash(tuple(coordinates)) % m\n",
        "    if m==None: return coordinates\n",
        "\n",
        "from math import floor, log\n",
        "from itertools import zip_longest\n",
        "\n",
        "def tiles (ihtORsize, numtilings, floats, ints=[], readonly=False):\n",
        "    \"\"\"returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n",
        "    qfloats = [floor(f*numtilings) for f in floats]\n",
        "    Tiles = []\n",
        "    for tiling in range(numtilings):\n",
        "        tilingX2 = tiling*2\n",
        "        coords = [tiling]\n",
        "        b = tiling\n",
        "        for q in qfloats:\n",
        "            coords.append( (q + b) // numtilings )\n",
        "            b += tilingX2\n",
        "        coords.extend(ints)\n",
        "        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n",
        "    return Tiles\n",
        "\n",
        "def tileswrap (ihtORsize, numtilings, floats, wrapwidths, ints=[], readonly=False):\n",
        "    \"\"\"returns num-tilings tile indices corresponding to the floats and ints, wrapping some floats\"\"\"\n",
        "    qfloats = [floor(f*numtilings) for f in floats]\n",
        "    Tiles = []\n",
        "    for tiling in range(numtilings):\n",
        "        tilingX2 = tiling*2\n",
        "        coords = [tiling]\n",
        "        b = tiling\n",
        "        for q, width in zip_longest(qfloats, wrapwidths):\n",
        "            c = (q + b%numtilings) // numtilings\n",
        "            coords.append(c%width if width else c)\n",
        "            b += tilingX2\n",
        "        coords.extend(ints)\n",
        "        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n",
        "    return Tiles\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rMBCrsLt1Zi9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ebe67c76b8e7e270a32f544f3fce5040",
          "grade": false,
          "grade_id": "cell-213dab5e4618e704",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ZWgALyAu0wu-"
      },
      "source": [
        "## Section 1: Create Tile Coding Helper Function\n",
        "\n",
        "In this section, we are going to build a tile coding class for our agent that will make it easier to make calls to our tile coder.\n",
        "\n",
        "Tile-coding is introduced in Section 9.5.4 of the textbook as a way to create features that can both provide good generalization and discrimination. We have already used it in our last programming assignment as well.\n",
        "\n",
        "Similar to the last programming assignment, we are going to make a function specific for tile coding for our Pendulum Swing-up environment. We will also use the [Tiles3 library](http://incompleteideas.net/tiles/tiles3.html).\n",
        "\n",
        "To get the tile coder working we need to:\n",
        "\n",
        "    1) create an index hash table using tc.IHT(),\n",
        "    2) scale the inputs for the tile coder based on number of tiles and range of values each input could take\n",
        "    3) call tc.tileswrap to get active tiles back.\n",
        "\n",
        "However, we need to make one small change to this tile coder.\n",
        "Note that in this environment the state space contains angle, which is between $[-\\pi, \\pi]$. If we tile-code this state space in the usual way, the agent may think the value of states corresponding to an angle of $-\\pi$ is very different from angle of $\\pi$ when in fact they are the same! To remedy this and allow generalization between angle $= -\\pi$ and angle $= \\pi$, we need to use **wrap tile coder**.\n",
        "\n",
        "The usage of wrap tile coder is almost identical to the original tile coder, except that we also need to provide the `wrapwidth` argument for the dimension we want to wrap over (hence only for angle, and `None` for angular velocity). More details of wrap tile coder is also provided in [Tiles3 library](http://incompleteideas.net/tiles/tiles3.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c7798475f36f714545d25cc19324b8d6",
          "grade": false,
          "grade_id": "cell-0aea119ff5ad24aa",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "0SZVDPJM0wu-"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Graded Cell\n",
        "# -----------\n",
        "class PendulumTileCoder:\n",
        "    def __init__(self, iht_size=4096, num_tilings=32, num_tiles=8):\n",
        "        \"\"\"\n",
        "        Initializes the MountainCar Tile Coder\n",
        "        Initializers:\n",
        "        iht_size -- int, the size of the index hash table, typically a power of 2\n",
        "        num_tilings -- int, the number of tilings\n",
        "        num_tiles -- int, the number of tiles. Here both the width and height of the tiles are the same\n",
        "\n",
        "        Class Variables:\n",
        "        self.iht -- tc.IHT, the index hash table that the tile coder will use\n",
        "        self.num_tilings -- int, the number of tilings the tile coder will use\n",
        "        self.num_tiles -- int, the number of tiles the tile coder will use\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_tilings = num_tilings\n",
        "        self.num_tiles = num_tiles\n",
        "        self.iht = tc.IHT(iht_size)\n",
        "\n",
        "    def get_tiles(self, angle, ang_vel):\n",
        "        \"\"\"\n",
        "        Takes in an angle and angular velocity from the pendulum environment\n",
        "        and returns a numpy array of active tiles.\n",
        "\n",
        "        Arguments:\n",
        "        angle -- float, the angle of the pendulum between -np.pi and np.pi\n",
        "        ang_vel -- float, the angular velocity of the agent between -2*np.pi and 2*np.pi\n",
        "\n",
        "        returns:\n",
        "        tiles -- np.array, active tiles\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        ### Use the ranges above and scale the angle and angular velocity between [0, 1]\n",
        "        # then multiply by the number of tiles so they are scaled between [0, self.num_tiles]\n",
        "\n",
        "        angle_scaled = 0\n",
        "        ang_vel_scaled = 0\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "\n",
        "        angle_scaled = (angle + np.pi) / (2 * np.pi) * self.num_tiles\n",
        "        ang_vel_scaled = (ang_vel + 2 * np.pi) / (4 * np.pi) * self.num_tiles\n",
        "        # ----------------\n",
        "\n",
        "        # Get tiles by calling tc.tileswrap method\n",
        "        # wrapwidths specify which dimension to wrap over and its wrapwidth\n",
        "        tiles = tc.tileswrap(self.iht, self.num_tilings, [angle_scaled, ang_vel_scaled], wrapwidths=[self.num_tiles, False])\n",
        "\n",
        "        return np.array(tiles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7a4fa9bdb9dc9e4475a08b429a512fa1",
          "grade": false,
          "grade_id": "cell-d7ec813a92c7a24a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8o19tymn0wvA"
      },
      "source": [
        "Run the following code to verify `PendulumTilecoder`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3dec4aa9b5d293b1d3f1809f58e68625",
          "grade": true,
          "grade_id": "cell-25f642d5c07d3914",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "S1W2-gfG0wvB"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "## Test Code for PendulumTileCoder ##\n",
        "# Your tile coder should also work for other num. tilings and num. tiles\n",
        "angles = np.linspace(-np.pi, np.pi, num=5)\n",
        "vels = np.linspace(-2 * np.pi, 2 * np.pi, num=5)\n",
        "test_obs = list(itertools.product(angles, vels))\n",
        "\n",
        "pdtc = PendulumTileCoder(iht_size=4096, num_tilings=8, num_tiles=2)\n",
        "\n",
        "result=[]\n",
        "for obs in test_obs:\n",
        "    angle, ang_vel = obs\n",
        "    tiles = pdtc.get_tiles(angle=angle, ang_vel=ang_vel)\n",
        "    result.append(tiles)\n",
        "\n",
        "expected = np.array([\n",
        "    [0, 1, 2, 3, 4, 5, 6, 7],\n",
        "    [0, 1, 8, 3, 9, 10, 6, 11],\n",
        "    [12, 13, 8, 14, 9, 10, 15, 11],\n",
        "    [12, 13, 16, 14, 17, 18, 15, 19],\n",
        "    [20, 21, 16, 22, 17, 18, 23, 19],\n",
        "    [0, 1, 2, 3, 24, 25, 26, 27],\n",
        "    [0, 1, 8, 3, 28, 29, 26, 30],\n",
        "    [12, 13, 8, 14, 28, 29, 31, 30],\n",
        "    [12, 13, 16, 14, 32, 33, 31, 34],\n",
        "    [20, 21, 16, 22, 32, 33, 35, 34],\n",
        "    [36, 37, 38, 39, 24, 25, 26, 27],\n",
        "    [36, 37, 40, 39, 28, 29, 26, 30],\n",
        "    [41, 42, 40, 43, 28, 29, 31, 30],\n",
        "    [41, 42, 44, 43, 32, 33, 31, 34],\n",
        "    [45, 46, 44, 47, 32, 33, 35, 34],\n",
        "    [36, 37, 38, 39, 4, 5, 6, 7],\n",
        "    [36, 37, 40, 39, 9, 10, 6, 11],\n",
        "    [41, 42, 40, 43, 9, 10, 15, 11],\n",
        "    [41, 42, 44, 43, 17, 18, 15, 19],\n",
        "    [45, 46, 44, 47, 17, 18, 23, 19],\n",
        "    [0, 1, 2, 3, 4, 5, 6, 7],\n",
        "    [0, 1, 8, 3, 9, 10, 6, 11],\n",
        "    [12, 13, 8, 14, 9, 10, 15, 11],\n",
        "    [12, 13, 16, 14, 17, 18, 15, 19],\n",
        "    [20, 21, 16, 22, 17, 18, 23, 19],\n",
        "])\n",
        "\n",
        "assert np.all(expected == np.array(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a454a99deb0ebf93f1211d1c9018d743",
          "grade": false,
          "grade_id": "cell-3137acdcbbf1cfc7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sJVwJwIc0wvB"
      },
      "source": [
        "## Section 2: Create Average Reward Softmax Actor-Critic Agent\n",
        "\n",
        "Now that we implemented PendulumTileCoder let's create the agent that interacts with the environment. We will implement the same average reward Actor-Critic algorithm presented in the videos.\n",
        "\n",
        "This agent has two components: an Actor and a Critic. The Actor learns a parameterized policy while the Critic learns a state-value function. The environment has discrete actions; your Actor implementation will use a softmax policy with exponentiated action-preferences. The Actor learns with the sample-based estimate for the gradient of the average reward objective. The Critic learns using the average reward version of the semi-gradient TD(0) algorithm.\n",
        "\n",
        "In this section, you will be implementing `agent_policy`, `agent_start`, `agent_step`, and `agent_end`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a4b30aeca061c0e8fffd2b9015e8af6a",
          "grade": false,
          "grade_id": "cell-517fd83fc10c06b2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Zj6AGJgM0wvC"
      },
      "source": [
        "## Section 2-1: Implement Helper Functions\n",
        "\n",
        "Let's first define a couple of useful helper functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "303cb299c47dbf6eddf5416b76587574",
          "grade": false,
          "grade_id": "cell-4a214ace7fefd6d7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "hZqyyqTM0wvD"
      },
      "source": [
        "## Section 2-1a: Compute Softmax Probability\n",
        "\n",
        "In this part you will implement `compute_softmax_prob`.\n",
        "\n",
        "This function computes softmax probability for all actions, given actor weights `actor_w` and active tiles `tiles`. This function will be later used in `agent_policy` to sample appropriate action.\n",
        "\n",
        "First, recall how the softmax policy is represented from state-action preferences: $\\large \\pi(a|s, \\mathbf{\\theta}) \\doteq \\frac{e^{h(s,a,\\mathbf{\\theta})}}{\\sum_{b}e^{h(s,b,\\mathbf{\\theta})}}$.\n",
        "\n",
        "**state-action preference** is defined as $h(s,a, \\mathbf{\\theta}) \\doteq \\mathbf{\\theta}^T \\mathbf{x}_h(s,a)$.\n",
        "\n",
        "Given active tiles `tiles` for state `s`, state-action preference $\\mathbf{\\theta}^T \\mathbf{x}_h(s,a)$ can be computed by `actor_w[a][tiles].sum()`.\n",
        "\n",
        "We will also use **exp-normalize trick**, in order to avoid possible numerical overflow.\n",
        "Consider the following:\n",
        "\n",
        "$\\large \\pi(a|s, \\mathbf{\\theta}) \\doteq \\frac{e^{h(s,a,\\mathbf{\\theta})}}{\\sum_{b}e^{h(s,b,\\mathbf{\\theta})}} = \\frac{e^{h(s,a,\\mathbf{\\theta}) - c} e^c}{\\sum_{b}e^{h(s,b,\\mathbf{\\theta}) - c} e^c} = \\frac{e^{h(s,a,\\mathbf{\\theta}) - c}}{\\sum_{b}e^{h(s,b,\\mathbf{\\theta}) - c}}$\n",
        "\n",
        "$\\pi(\\cdot|s, \\mathbf{\\theta})$ is shift-invariant, and the policy remains the same when we subtract a constant $c \\in \\mathbb{R}$ from state-action preferences.\n",
        "\n",
        "Normally we use $c = \\max_b h(s,b, \\mathbf{\\theta})$, to prevent any overflow due to exponentiating large numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9359abf8f5989b07767b67e08a051cf8",
          "grade": false,
          "grade_id": "cell-8e687ecd856dcacd",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "35i2hRcA0wvE"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Graded Cell\n",
        "# -----------\n",
        "\n",
        "def compute_softmax_prob(actor_w, tiles):\n",
        "    \"\"\"\n",
        "    Computes softmax probability for all actions\n",
        "\n",
        "    Args:\n",
        "    actor_w - np.array, an array of actor weights\n",
        "    tiles - np.array, an array of active tiles\n",
        "\n",
        "    Returns:\n",
        "    softmax_prob - np.array, an array of size equal to num. actions, and sums to 1.\n",
        "    \"\"\"\n",
        "\n",
        "    # First compute the list of state-action preferences (1~2 lines)\n",
        "    # state_action_preferences = ? (list of size 3)\n",
        "    state_action_preferences = []\n",
        "    # ----------------\n",
        "    # your code here\n",
        "    #print(actor_w)\n",
        "    #print(\"tiles\", tiles)\n",
        "    state_action_preferences = np.zeros(3) #we have 3 actions to select from\n",
        "    for action in range(3):\n",
        "        #for each action sum the updated weights, which are actully the action 'value'\n",
        "        state_action_preferences[action] = np.sum(actor_w[action][tiles])\n",
        "\n",
        "    #print('state_action_preferences',state_action_preferences)\n",
        "    # ----------------\n",
        "\n",
        "    # Set the constant c by finding the maximum of state-action preferences (use np.max) (1 line)\n",
        "    # c = ? (float)\n",
        "    # ----------------\n",
        "    # your code here\n",
        "    c = np.max(state_action_preferences) #find the best action\n",
        "    #print('best action', c)\n",
        "    # ----------------\n",
        "\n",
        "    # Compute the numerator by subtracting c from state-action preferences and exponentiating it (use np.exp) (1 line)\n",
        "    # numerator = ? (list of size 3)\n",
        "    # ----------------\n",
        "    # your code here\n",
        "    numerator =  np.exp(state_action_preferences - c)\n",
        "    #print('numerator',numerator)\n",
        "    # ----------------\n",
        "\n",
        "    # Next compute the denominator by summing the values in the numerator (use np.sum) (1 line)\n",
        "    # denominator = ? (float)\n",
        "    # ----------------\n",
        "    # your code here\n",
        "    denominator = np.sum(numerator)\n",
        "    #print('denominator',denominator)\n",
        "    # ----------------\n",
        "\n",
        "\n",
        "    # Create a probability array by dividing each element in numerator array by denominator (1 line)\n",
        "    # We will store this probability array in self.softmax_prob as it will be useful later when updating the Actor\n",
        "    # softmax_prob = ? (list of size 3)\n",
        "    # ----------------\n",
        "    # your code here\n",
        "    softmax_prob = numerator / denominator\n",
        "    # ----------------\n",
        "\n",
        "    return softmax_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "761889b9695a5cae6f27e56d57bd9bcf",
          "grade": false,
          "grade_id": "cell-42024f3263f17369",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "rrw3d04R0wvF"
      },
      "source": [
        "Run the following code to verify `compute_softmax_prob`.\n",
        "\n",
        "We will test the method by building a softmax policy from state-action preferences [-1,1,2].\n",
        "\n",
        "The sampling probability should then roughly match $[\\frac{e^{-1}}{e^{-1}+e^1+e^2}, \\frac{e^{1}}{e^{-1}+e^1+e^2}, \\frac{e^2}{e^{-1}+e^1+e^2}] \\approx$ [0.0351, 0.2595, 0.7054]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c029cf63786e01653c60f7b2130fbba2",
          "grade": true,
          "grade_id": "cell-b27241568a857869",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "LJ3wmrKA0wvF",
        "outputId": "2ea4a790-a5b3-424f-b1e4-c60340e53057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "softmax probability: [0.03511903 0.25949646 0.70538451]\n"
          ]
        }
      ],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "# set tile-coder\n",
        "iht_size = 4096\n",
        "num_tilings = 8\n",
        "num_tiles = 8\n",
        "test_tc = PendulumTileCoder(iht_size=iht_size, num_tilings=num_tilings, num_tiles=num_tiles)\n",
        "\n",
        "num_actions = 3\n",
        "actions = list(range(num_actions))\n",
        "actor_w = np.zeros((len(actions), iht_size))\n",
        "\n",
        "# setting actor weights such that state-action preferences are always [-1, 1, 2]\n",
        "actor_w[0] = -1./num_tilings\n",
        "actor_w[1] = 1./num_tilings\n",
        "actor_w[2] = 2./num_tilings\n",
        "\n",
        "# obtain active_tiles from state\n",
        "state = [-np.pi, 0.]\n",
        "angle, ang_vel = state\n",
        "active_tiles = test_tc.get_tiles(angle, ang_vel)\n",
        "\n",
        "# compute softmax probability\n",
        "softmax_prob = compute_softmax_prob(actor_w, active_tiles)\n",
        "print('softmax probability: {}'.format(softmax_prob))\n",
        "\n",
        "assert np.allclose(softmax_prob, [0.03511903, 0.25949646, 0.70538451])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4615f9dd8c5d414de6c4486197d0cf16",
          "grade": false,
          "grade_id": "cell-7d31bba88af1cbaf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "TtPJjo5O0wvH"
      },
      "source": [
        "## Section 2-2: Implement Agent Methods\n",
        "\n",
        "Let's first define methods that initialize the agent. `agent_init()` initializes all the variables that the agent will need.\n",
        "\n",
        "Now that we have implemented helper functions, let's create an agent. In this part, you will implement `agent_start()` and `agent_step()`. We do not need to implement `agent_end()` because there is no termination in our continuing task.\n",
        "\n",
        "`compute_softmax_prob()` is used in `agent_policy()`, which in turn will be used in `agent_start()` and `agent_step()`. We have implemented `agent_policy()` for you.\n",
        "\n",
        "When performing updates to the Actor and Critic, recall their respective updates in the Actor-Critic algorithm video.\n",
        "\n",
        "We approximate $q_\\pi$ in the Actor update using one-step bootstrapped return($R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, \\mathbf{w})$) subtracted by current state-value($\\hat{v}(S_{t}, \\mathbf{w})$), equivalent to TD error $\\delta$.\n",
        "\n",
        "$\\delta_t = R_{t+1} - \\bar{R} + \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_{t}, \\mathbf{w}) \\hspace{6em} (1)$\n",
        "\n",
        "**Average Reward update rule**: $\\bar{R} \\leftarrow \\bar{R} + \\alpha^{\\bar{R}}\\delta \\hspace{4.3em} (2)$\n",
        "\n",
        "**Critic weight update rule**: $\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha^{\\mathbf{w}}\\delta\\nabla \\hat{v}(s,\\mathbf{w}) \\hspace{2.5em} (3)$\n",
        "\n",
        "**Actor weight update rule**: $\\mathbf{\\theta} \\leftarrow \\mathbf{\\theta} + \\alpha^{\\mathbf{\\theta}}\\delta\\nabla ln \\pi(A|S,\\mathbf{\\theta}) \\hspace{1.4em} (4)$\n",
        "\n",
        "\n",
        "However, since we are using linear function approximation and parameterizing a softmax policy, the above update rule can be further simplified using:\n",
        "\n",
        "$\\nabla \\hat{v}(s,\\mathbf{w}) = \\mathbf{x}(s) \\hspace{14.2em} (5)$\n",
        "\n",
        "$\\nabla ln \\pi(A|S,\\mathbf{\\theta}) = \\mathbf{x}_h(s,a) - \\sum_b \\pi(b|s, \\mathbf{\\theta})\\mathbf{x}_h(s,b) \\hspace{3.3em} (6)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7d2aa6794604c259643d6bad07cc6b6f",
          "grade": false,
          "grade_id": "cell-1a5afc4aed047144",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "RPrf9jix0wvH"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Graded Cell\n",
        "# -----------\n",
        "\n",
        "class ActorCriticSoftmaxAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        self.rand_generator = None\n",
        "\n",
        "        self.actor_step_size = None\n",
        "        self.critic_step_size = None\n",
        "        self.avg_reward_step_size = None\n",
        "\n",
        "        self.tc = None\n",
        "\n",
        "        self.avg_reward = None\n",
        "        self.critic_w = None\n",
        "        self.actor_w = None\n",
        "\n",
        "        self.actions = None\n",
        "\n",
        "        self.softmax_prob = None\n",
        "        self.prev_tiles = None\n",
        "        self.last_action = None\n",
        "\n",
        "    def agent_init(self, agent_info={}):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\n",
        "\n",
        "        Set parameters needed to setup the semi-gradient TD(0) state aggregation agent.\n",
        "\n",
        "        Assume agent_info dict contains:\n",
        "        {\n",
        "            \"iht_size\": int\n",
        "            \"num_tilings\": int,\n",
        "            \"num_tiles\": int,\n",
        "            \"actor_step_size\": float,\n",
        "            \"critic_step_size\": float,\n",
        "            \"avg_reward_step_size\": float,\n",
        "            \"num_actions\": int,\n",
        "            \"seed\": int\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        # set random seed for each run\n",
        "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n",
        "\n",
        "        iht_size = agent_info.get(\"iht_size\")\n",
        "        num_tilings = agent_info.get(\"num_tilings\")\n",
        "        num_tiles = agent_info.get(\"num_tiles\")\n",
        "\n",
        "        # initialize self.tc to the tile coder we created\n",
        "        self.tc = PendulumTileCoder(iht_size=iht_size, num_tilings=num_tilings, num_tiles=num_tiles)\n",
        "\n",
        "        # set step-size accordingly (we normally divide actor and critic step-size by num. tilings (p.217-218 of textbook))\n",
        "        self.actor_step_size = agent_info.get(\"actor_step_size\")/num_tilings\n",
        "        self.critic_step_size = agent_info.get(\"critic_step_size\")/num_tilings\n",
        "        self.avg_reward_step_size = agent_info.get(\"avg_reward_step_size\")\n",
        "\n",
        "        self.actions = list(range(agent_info.get(\"num_actions\")))\n",
        "\n",
        "        # Set initial values of average reward, actor weights, and critic weights\n",
        "        # We initialize actor weights to three times the iht_size.\n",
        "        # Recall this is because we need to have one set of weights for each of the three actions.\n",
        "        self.avg_reward = 0.0\n",
        "        self.actor_w = np.zeros((len(self.actions), iht_size))\n",
        "        self.critic_w = np.zeros(iht_size)\n",
        "\n",
        "        self.softmax_prob = None\n",
        "        self.prev_tiles = None\n",
        "        self.last_action = None\n",
        "\n",
        "    def agent_policy(self, active_tiles):\n",
        "        \"\"\" policy of the agent\n",
        "        Args:\n",
        "            active_tiles (Numpy array): active tiles returned by tile coder\n",
        "\n",
        "        Returns:\n",
        "            The action selected according to the policy\n",
        "        \"\"\"\n",
        "\n",
        "        # compute softmax probability\n",
        "        softmax_prob = compute_softmax_prob(self.actor_w, active_tiles)\n",
        "\n",
        "        # Sample action from the softmax probability array\n",
        "        # self.rand_generator.choice() selects an element from the array with the specified probability\n",
        "        chosen_action = self.rand_generator.choice(self.actions, p=softmax_prob)\n",
        "\n",
        "        # save softmax_prob as it will be useful later when updating the Actor\n",
        "        self.softmax_prob = softmax_prob\n",
        "\n",
        "        return chosen_action\n",
        "\n",
        "    def agent_start(self, state):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            state (Numpy array): the state from the environment's env_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "\n",
        "        angle, ang_vel = state\n",
        "\n",
        "        ### Use self.tc to get active_tiles using angle and ang_vel (2 lines)\n",
        "        # set current_action by calling self.agent_policy with active_tiles\n",
        "        # active_tiles = ?\n",
        "        # current_action = ?\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        active_tiles = self.tc.get_tiles(angle, ang_vel)\n",
        "        current_action = self.agent_policy(active_tiles)\n",
        "        # ----------------\n",
        "\n",
        "        self.last_action = current_action\n",
        "        self.prev_tiles = np.copy(active_tiles)\n",
        "\n",
        "        return self.last_action\n",
        "\n",
        "\n",
        "    def agent_step(self, reward, state):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            state (Numpy array): the state from the environment's step based on\n",
        "                                where the agent ended up after the\n",
        "                                last step.\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "\n",
        "        angle, ang_vel = state\n",
        "\n",
        "        ### Use self.tc to get active_tiles using angle and ang_vel (1 line)\n",
        "        # active_tiles = ?\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        active_tiles = self.tc.get_tiles(angle, ang_vel)\n",
        "        # ----------------\n",
        "\n",
        "        ### Compute delta using Equation (1) (1 line)\n",
        "        # delta = ?\n",
        "        # ----------------\n",
        "        # your code here 𝛿𝑡=𝑅𝑡+1−𝑅¯+𝑣̂ (𝑆𝑡+1,𝐰)−𝑣̂ (𝑆𝑡,𝐰)\n",
        "        delta = reward - self.avg_reward\n",
        "        # ----------------\n",
        "\n",
        "        ### update average reward using Equation (2) (1 line)\n",
        "        # self.avg_reward += ?\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        self.avg_reward += self.avg_reward_step_size * delta\n",
        "        # ----------------\n",
        "\n",
        "        # update critic weights using Equation (3) and (5) (1 line)\n",
        "        # self.critic_w[self.prev_tiles] += ?\n",
        "        # ----------------\n",
        "        # your code here 𝛼𝐰𝛿∇𝑣̂ (𝑠,𝐰)\n",
        "        self.critic_w[self.prev_tiles] += self.critic_step_size * delta * 1\n",
        "        # ----------------\n",
        "\n",
        "        # update actor weights using Equation (4) and (6)\n",
        "        # We use self.softmax_prob saved from the previous timestep\n",
        "        # We leave it as an exercise to verify that the code below corresponds to the equation.\n",
        "        for a in self.actions:\n",
        "            if a == self.last_action:\n",
        "                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (1 - self.softmax_prob[a])\n",
        "            else:\n",
        "                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (0 - self.softmax_prob[a])\n",
        "\n",
        "        ### set current_action by calling self.agent_policy with active_tiles (1 line)\n",
        "        # current_action = ?\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        current_action = self.agent_policy(active_tiles)\n",
        "        # ----------------\n",
        "\n",
        "        self.prev_tiles = active_tiles\n",
        "        self.last_action = current_action\n",
        "\n",
        "        return self.last_action\n",
        "\n",
        "\n",
        "    def agent_message(self, message):\n",
        "        if message == 'get avg reward':\n",
        "            return self.avg_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a1433cfb90cce3e1fd4183cb96150aca",
          "grade": false,
          "grade_id": "cell-fad2d97e598fa22a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "CrkK2lxI0wvK"
      },
      "source": [
        "Run the following code to verify `agent_start()`.\n",
        "Although there is randomness due to `self.rand_generator.choice()` in `agent_policy()`, we control the seed so your output should match the expected output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bc38580596ba40adffc54281e60f990b",
          "grade": true,
          "grade_id": "cell-a7d7210fe9299556",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "FForpZNi0wvK",
        "outputId": "55d0b512-da38-4057-f233-7b66f0fddc0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agent active_tiles: [0 1 2 3 4 5 6 7]\n",
            "agent selected action: 2\n"
          ]
        }
      ],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "agent_info = {\n",
        "    \"iht_size\": 4096,\n",
        "    \"num_tilings\": 8,\n",
        "    \"num_tiles\": 8,\n",
        "    \"actor_step_size\": 1e-1,\n",
        "    \"critic_step_size\": 1e-0,\n",
        "    \"avg_reward_step_size\": 1e-2,\n",
        "    \"num_actions\": 3,\n",
        "    \"seed\": 99,\n",
        "}\n",
        "\n",
        "test_agent = ActorCriticSoftmaxAgent()\n",
        "test_agent.agent_init(agent_info)\n",
        "\n",
        "state = [-np.pi, 0.]\n",
        "\n",
        "test_agent.agent_start(state)\n",
        "\n",
        "assert np.all(test_agent.prev_tiles == [0, 1, 2, 3, 4, 5, 6, 7])\n",
        "assert test_agent.last_action == 2\n",
        "\n",
        "print(\"agent active_tiles: {}\".format(test_agent.prev_tiles))\n",
        "print(\"agent selected action: {}\".format(test_agent.last_action))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fba7f24b5227e2b1ec26b3c8a4a36956",
          "grade": false,
          "grade_id": "cell-ac39ccab6b2747e4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-eeMhSaP0wvL"
      },
      "source": [
        "Run the following code to verify `agent_step()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "331d10b308ac8e4820239ada569bbd29",
          "grade": true,
          "grade_id": "cell-f53c4c744932d219",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "mU35EJmp0wvL",
        "outputId": "576de8b6-596c-4f04-c102-3df36e05bb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agent next_action: 1\n",
            "agent avg reward: -0.03139092653589793\n",
            "\n",
            "agent first 10 values of actor weights[0]: \n",
            "[0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.01307955\n",
            " 0.01307955 0.01307955 0.         0.        ]\n",
            "\n",
            "agent first 10 values of actor weights[1]: \n",
            "[0.01307955 0.01307955 0.01307955 0.01307955 0.01307955 0.01307955\n",
            " 0.01307955 0.01307955 0.         0.        ]\n",
            "\n",
            "agent first 10 values of actor weights[2]: \n",
            "[-0.02615911 -0.02615911 -0.02615911 -0.02615911 -0.02615911 -0.02615911\n",
            " -0.02615911 -0.02615911  0.          0.        ]\n",
            "\n",
            "agent first 10 values of critic weights: \n",
            "[-0.39238658 -0.39238658 -0.39238658 -0.39238658 -0.39238658 -0.39238658\n",
            " -0.39238658 -0.39238658  0.          0.        ]\n"
          ]
        }
      ],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "# Make sure agent_start() and agent_policy() are working correctly first.\n",
        "# agent_step() should work correctly for other arbitrary state transitions in addition to this test case.\n",
        "\n",
        "env_info = {\"seed\": 99}\n",
        "agent_info = {\n",
        "    \"iht_size\": 4096,\n",
        "    \"num_tilings\": 8,\n",
        "    \"num_tiles\": 8,\n",
        "    \"actor_step_size\": 1e-1,\n",
        "    \"critic_step_size\": 1e-0,\n",
        "    \"avg_reward_step_size\": 1e-2,\n",
        "    \"num_actions\": 3,\n",
        "    \"seed\": 99,\n",
        "}\n",
        "\n",
        "rl_glue = RLGlue(PendulumEnvironment, ActorCriticSoftmaxAgent)\n",
        "rl_glue.rl_init(agent_info, env_info)\n",
        "\n",
        "# start env/agent\n",
        "rl_glue.rl_start()\n",
        "rl_glue.rl_step()\n",
        "\n",
        "# simple alias\n",
        "agent = rl_glue.agent\n",
        "\n",
        "print(\"agent next_action: {}\".format(agent.last_action))\n",
        "print(\"agent avg reward: {}\\n\".format(agent.avg_reward))\n",
        "\n",
        "assert agent.last_action == 1\n",
        "assert agent.avg_reward == -0.03139092653589793\n",
        "\n",
        "print(\"agent first 10 values of actor weights[0]: \\n{}\\n\".format(agent.actor_w[0][:10]))\n",
        "print(\"agent first 10 values of actor weights[1]: \\n{}\\n\".format(agent.actor_w[1][:10]))\n",
        "print(\"agent first 10 values of actor weights[2]: \\n{}\\n\".format(agent.actor_w[2][:10]))\n",
        "print(\"agent first 10 values of critic weights: \\n{}\".format(agent.critic_w[:10]))\n",
        "\n",
        "assert np.allclose(agent.actor_w[0][:10], [0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0., 0.])\n",
        "assert np.allclose(agent.actor_w[1][:10], [0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0., 0.])\n",
        "assert np.allclose(agent.actor_w[2][:10], [-0.02615911, -0.02615911, -0.02615911, -0.02615911, -0.02615911, -0.02615911, -0.02615911, -0.02615911, 0., 0.])\n",
        "\n",
        "assert np.allclose(agent.critic_w[:10], [-0.39238658, -0.39238658, -0.39238658, -0.39238658, -0.39238658, -0.39238658, -0.39238658, -0.39238658, 0., 0.])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "34b54ca88f769153b50bb9cf7a41b249",
          "grade": false,
          "grade_id": "cell-bac2950fd815375d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-dUVpLdU0wvM"
      },
      "source": [
        "## Section 3: Run Experiment\n",
        "\n",
        "Now that we've implemented all the components of environment and agent, let's run an experiment!\n",
        "We want to see whether our agent is successful at learning the optimal policy of balancing the pendulum upright. We will plot total return over time, as well as the exponential average of the reward over time. We also do multiple runs in order to be confident about our results.  \n",
        "\n",
        "The experiment/plot code is provided in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f5091c85ef2c69a7dc5992f593acc4d9",
          "grade": false,
          "grade_id": "cell-8d55fd86b9fc3769",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "dTH8Q9CX0wvM"
      },
      "outputs": [],
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "\n",
        "# Define function to run experiment\n",
        "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
        "\n",
        "    rl_glue = RLGlue(environment, agent)\n",
        "\n",
        "    # sweep agent parameters\n",
        "    for num_tilings in agent_parameters['num_tilings']:\n",
        "        for num_tiles in agent_parameters[\"num_tiles\"]:\n",
        "            for actor_ss in agent_parameters[\"actor_step_size\"]:\n",
        "                for critic_ss in agent_parameters[\"critic_step_size\"]:\n",
        "                    for avg_reward_ss in agent_parameters[\"avg_reward_step_size\"]:\n",
        "\n",
        "                        env_info = {}\n",
        "                        agent_info = {\"num_tilings\": num_tilings,\n",
        "                                      \"num_tiles\": num_tiles,\n",
        "                                      \"actor_step_size\": actor_ss,\n",
        "                                      \"critic_step_size\": critic_ss,\n",
        "                                      \"avg_reward_step_size\": avg_reward_ss,\n",
        "                                      \"num_actions\": agent_parameters[\"num_actions\"],\n",
        "                                      \"iht_size\": agent_parameters[\"iht_size\"]}\n",
        "\n",
        "                        # results to save\n",
        "                        return_per_step = np.zeros((experiment_parameters[\"num_runs\"], experiment_parameters[\"max_steps\"]))\n",
        "                        exp_avg_reward_per_step = np.zeros((experiment_parameters[\"num_runs\"], experiment_parameters[\"max_steps\"]))\n",
        "\n",
        "                        # using tqdm we visualize progress bars\n",
        "                        for run in tqdm(range(1, experiment_parameters[\"num_runs\"]+1)):\n",
        "                            env_info[\"seed\"] = run\n",
        "                            agent_info[\"seed\"] = run\n",
        "\n",
        "                            rl_glue.rl_init(agent_info, env_info)\n",
        "                            rl_glue.rl_start()\n",
        "\n",
        "                            num_steps = 0\n",
        "                            total_return = 0.\n",
        "                            return_arr = []\n",
        "\n",
        "                            # exponential average reward without initial bias\n",
        "                            exp_avg_reward = 0.0\n",
        "                            exp_avg_reward_ss = 0.01\n",
        "                            exp_avg_reward_normalizer = 0\n",
        "\n",
        "                            while num_steps < experiment_parameters['max_steps']:\n",
        "                                num_steps += 1\n",
        "\n",
        "                                rl_step_result = rl_glue.rl_step()\n",
        "\n",
        "                                reward = rl_step_result[0]\n",
        "                                total_return += reward\n",
        "                                return_arr.append(reward)\n",
        "                                avg_reward = rl_glue.rl_agent_message(\"get avg reward\")\n",
        "\n",
        "                                exp_avg_reward_normalizer = exp_avg_reward_normalizer + exp_avg_reward_ss * (1 - exp_avg_reward_normalizer)\n",
        "                                ss = exp_avg_reward_ss / exp_avg_reward_normalizer\n",
        "                                exp_avg_reward += ss * (reward - exp_avg_reward)\n",
        "\n",
        "                                return_per_step[run-1][num_steps-1] = total_return\n",
        "                                exp_avg_reward_per_step[run-1][num_steps-1] = exp_avg_reward\n",
        "\n",
        "                        if not os.path.exists('results'):\n",
        "                            os.makedirs('results')\n",
        "\n",
        "                        save_name = \"ActorCriticSoftmax_tilings_{}_tiledim_{}_actor_ss_{}_critic_ss_{}_avg_reward_ss_{}\".format(num_tilings, num_tiles, actor_ss, critic_ss, avg_reward_ss)\n",
        "                        total_return_filename = \"results/{}_total_return.npy\".format(save_name)\n",
        "                        exp_avg_reward_filename = \"results/{}_exp_avg_reward.npy\".format(save_name)\n",
        "\n",
        "                        np.save(total_return_filename, return_per_step)\n",
        "                        np.save(exp_avg_reward_filename, exp_avg_reward_per_step)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "986f2c1efbd936813ceac9d1749f3feb",
          "grade": false,
          "grade_id": "cell-af8391c13e4be3c8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "AfzE5vSF0wvO"
      },
      "source": [
        "## Section 3-1: Run Experiment with 32 tilings, size 8x8\n",
        "\n",
        "We will first test our implementation using 32 tilings, of size 8x8. We saw from the earlier assignment using tile-coding that many tilings promote fine discrimination, and broad tiles allows more generalization.\n",
        "We conducted a wide sweep of meta-parameters in order to find the best meta-parameters for our Pendulum Swing-up task.\n",
        "\n",
        "We swept over the following range of meta-parameters and the best meta-parameter is boldfaced below:\n",
        "\n",
        "actor step-size: $\\{\\frac{2^{-6}}{32}, \\frac{2^{-5}}{32}, \\frac{2^{-4}}{32}, \\frac{2^{-3}}{32}, \\mathbf{\\frac{2^{-2}}{32}}, \\frac{2^{-1}}{32}, \\frac{2^{0}}{32}, \\frac{2^{1}}{32}\\}$\n",
        "\n",
        "critic step-size: $\\{\\frac{2^{-4}}{32}, \\frac{2^{-3}}{32}, \\frac{2^{-2}}{32}, \\frac{2^{-1}}{32}, \\frac{2^{0}}{32}, \\mathbf{\\frac{2^{1}}{32}}, \\frac{3}{32}, \\frac{2^{2}}{32}\\}$\n",
        "\n",
        "avg reward step-size: $\\{2^{-11}, 2^{-10} , 2^{-9} , 2^{-8}, 2^{-7}, \\mathbf{2^{-6}}, 2^{-5}, 2^{-4}, 2^{-3}, 2^{-2}\\}$  \n",
        "\n",
        "\n",
        "We will do 50 runs using the above best meta-parameter setting to verify your agent.\n",
        "Note that running the experiment cell below will take **_approximately 5 min_**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9f76e04859fa6d09d18caca6b89bae8f",
          "grade": false,
          "grade_id": "cell-4d1df8adaf0d23c6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "bHRADX5v0wvO",
        "outputId": "501e1837-494b-4fe8-95a2-ca9bea64272a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [02:42<00:00,  3.26s/it]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x1008 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "\n",
        "#### Run Experiment\n",
        "\n",
        "# Experiment parameters\n",
        "experiment_parameters = {\n",
        "    \"max_steps\" : 20000,\n",
        "    \"num_runs\" : 50\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "environment_parameters = {}\n",
        "\n",
        "# Agent parameters\n",
        "# Each element is an array because we will be later sweeping over multiple values\n",
        "# actor and critic step-sizes are divided by num. tilings inside the agent\n",
        "agent_parameters = {\n",
        "    \"num_tilings\": [32],\n",
        "    \"num_tiles\": [8],\n",
        "    \"actor_step_size\": [2**(-2)],\n",
        "    \"critic_step_size\": [2**1],\n",
        "    \"avg_reward_step_size\": [2**(-6)],\n",
        "    \"num_actions\": 3,\n",
        "    \"iht_size\": 4096\n",
        "}\n",
        "\n",
        "current_env = PendulumEnvironment\n",
        "current_agent = ActorCriticSoftmaxAgent\n",
        "\n",
        "\n",
        "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n",
        "plot_script.plot_result(agent_parameters, 'results')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bcffcef5262c887f5a94191b7c37915a",
          "grade": false,
          "grade_id": "cell-146b310087d1a4f5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "RUthCN4c0wvP"
      },
      "source": [
        "Run the following code to verify your experimental result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "32a257715ac93c6b5c648b875f396e9c",
          "grade": false,
          "grade_id": "cell-60ba9335a60d9f20",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vIEtQ8Da0wvP",
        "outputId": "08b9e338-7083-4838-c697-87e973dab004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your experiment results does not match with ours. Please check if you have implemented all methods correctly.\n"
          ]
        }
      ],
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "\n",
        "## Test Code for experimental result ##\n",
        "filename = 'ActorCriticSoftmax_tilings_32_tiledim_8_actor_ss_0.25_critic_ss_2_avg_reward_ss_0.015625_exp_avg_reward'\n",
        "agent_exp_avg_reward = np.load('results/{}.npy'.format(filename), allow_pickle=True)\n",
        "result_med = np.median(agent_exp_avg_reward, axis=0)\n",
        "\n",
        "answer_range = np.load('correct_npy/exp_avg_reward_answer_range.npy', allow_pickle=True)\n",
        "upper_bound = answer_range.item()['upper-bound']\n",
        "lower_bound = answer_range.item()['lower-bound']\n",
        "\n",
        "# check if result is within answer range\n",
        "all_correct = np.all(result_med <= upper_bound) and np.all(result_med >= lower_bound)\n",
        "\n",
        "if all_correct:\n",
        "    print(\"Your experiment results are correct!\")\n",
        "else:\n",
        "    print(\"Your experiment results does not match with ours. Please check if you have implemented all methods correctly.\")\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a7d8ad3d50bae004227329037368dabc",
          "grade": false,
          "grade_id": "cell-427b974ad2550ce7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "HliY2LKa0wvP"
      },
      "source": [
        "## Section 3-2: Performance Metric and Meta-Parameter Sweeps\n",
        "\n",
        "\n",
        "### Performance Metric\n",
        "\n",
        "To evaluate performance, we plotted both the return and exponentially weighted average reward over time.\n",
        "\n",
        "In the first plot, the return is negative because the reward is negative at every state except when the pendulum is in the upright position. As the policy improves over time, the agent accumulates less negative reward, and thus the return decreases slowly. Towards the end the slope is almost flat indicating the policy has stabilized to a good policy. When using this plot however, it can be difficult to distinguish whether it has learned an optimal policy. The near-optimal policy in this Pendulum Swing-up Environment is to maintain the pendulum in the upright position indefinitely, getting near 0 reward at each time step. We would have to examine the slope of the curve but it can be hard to compare the slope of different curves.\n",
        "\n",
        "The second plot using exponential average reward gives a better visualization. We can see that towards the end the value is near 0, indicating it is getting near 0 reward at each time step. Here, the exponentially weighted average reward shouldn't be confused with the agent’s internal estimate of the average reward. To be more specific, we used an exponentially weighted average of the actual reward without initial bias (Refer to Exercise 2.7 from the textbook (p.35) to read more about removing the initial bias). If we used sample averages instead, later rewards would have decreasing impact on the average and would not be able to represent the agent's performance with respect to its current policy effectively.\n",
        "\n",
        "It is easier to see whether the agent has learned a good policy in the second plot than the first plot. If the learned policy is optimal, the exponential average reward would be close to 0.\n",
        "\n",
        "Furthermore, how did we pick the best meta-parameter from the sweeps? A common method would be to pick the meta-parameter that results in the largest Area Under the Curve (AUC). However, this is not always what we want. We want to find a set of meta-parameters that learns a good final policy. When using AUC as the criteria, we may pick meta-parameters that allows the agent to learn fast but converge to a worse policy. In our case, we selected the meta-parameter setting that obtained the most exponential average reward over the last 5000 time steps.\n",
        "\n",
        "\n",
        "### Parameter Sensitivity\n",
        "\n",
        "In addition to finding the best meta-parameters it is also equally important to plot **parameter sensitivity curves** to understand how our algorithm behaves.\n",
        "\n",
        "In our simulated Pendulum problem, we can extensively test our agent with different meta-parameter configurations but it would be quite expensive to do so in real life. Parameter sensitivity curves can provide us insight into how our algorithms might behave in general. It can help us identify a good range of each meta-parameters as well as how sensitive the performance is with respect to each meta-parameter.\n",
        "\n",
        "Here are the sensitivity curves for the three step-sizes we swept over:\n",
        "\n",
        "<img src=\"data/sensitivity_combined.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
        "\n",
        "On the y-axis we use the performance measure, which is the average of the exponential average reward over the 5000 time steps, averaged over 50 different runs. On the x-axis is the meta-parameter we are testing. For the given meta-parameter, the remaining meta-parameters are chosen such that it obtains the best performance.\n",
        "\n",
        "The curves are quite rounded, indicating the agent performs well for these wide range of values. It indicates that the agent is not too sensitive to these meta-parameters. Furthermore, looking at the y-axis values we can observe that average reward step-size is particularly less sensitive than actor step-size and critic step-size.\n",
        "\n",
        "But how do we know that we have sufficiently covered a wide range of meta-parameters? It is important that the best value is not on the edge but in the middle of the meta-parameter sweep range in these sensitivity curves. Otherwise this may indicate that there could be better meta-parameter values that we did not sweep over."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5297e29ae9abf6cd1d9f2aac5bf53df2",
          "grade": false,
          "grade_id": "cell-75e8b239dcff4058",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "pMVWv3JP0wvQ"
      },
      "source": [
        "## Wrapping up\n",
        "\n",
        "### **Congratulations!** You have successfully implemented Course 3 Programming Assignment 4.\n",
        "\n",
        "\n",
        "You have implemented your own **Average Reward Actor-Critic with Softmax Policy** agent in the Pendulum Swing-up Environment. You implemented the environment based on information about the state/action space and transition dynamics. Furthermore, you have learned how to implement an agent in a continuing task using the average reward formulation. We parameterized the policy using softmax of action-preferences over discrete action spaces, and used Actor-Critic to learn the policy.\n",
        "\n",
        "\n",
        "To summarize, you have learned how to:\n",
        "    1. Implement softmax actor-critic agent on a continuing task using the average reward formulation.\n",
        "    2. Understand how to parameterize the policy as a function to learn, in a discrete action environment.\n",
        "    3. Understand how to (approximately) sample the gradient of this objective to update the actor.\n",
        "    4. Understand how to update the critic using differential TD error."
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "prediction-control-function-approximation",
      "graded_item_id": "bHUHt",
      "launcher_item_id": "Igqsy"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
