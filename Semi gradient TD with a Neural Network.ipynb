{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9f4e1f41e5a3745101371efa1ef4e091",
          "grade": false,
          "grade_id": "cell-4774adbee156b2dc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "P2Rz62uHG_CX"
      },
      "source": [
        "# Assignment 2 - Semi-gradient TD with a Neural Network\n",
        "\n",
        "Welcome to Course 3 Programming Assignment 2. In the previous assignment, you implemented semi-gradient TD with State Aggregation for solving a **policy evaluation task**. In this assignment, you will implement **semi-gradient TD with a simple Neural Network** and use it for the same policy evaluation problem.\n",
        "\n",
        "You will implement an agent to evaluate a fixed policy on the 500-State Randomwalk. As you may remember from the previous assignment, the 500-state Randomwalk includes 500 states. Each episode begins with the agent at the center and terminates when the agent goes far left beyond state 1 or far right beyond state 500. At each time step, the agent selects to move either left or right with equal probability. The environment determines how much the agent moves in the selected direction.\n",
        "\n",
        "**In this assignment, you will:**\n",
        "- Implement stochastic gradient descent method for state-value prediction.\n",
        "- Implement semi-gradient TD with a neural network as the function approximator and Adam algorithm.\n",
        "- Compare performance of semi-gradient TD with a neural network and semi-gradient TD with tile-coding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "03465230d30671144fb59aa611d66367",
          "grade": false,
          "grade_id": "cell-79a3581e973fbe78",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "IgHMGMTaG_CZ"
      },
      "source": [
        "## Packages\n",
        "We import the following libraries that are required for this assignment:\n",
        "\n",
        "- [numpy](www.numpy.org) : Fundamental package for scientific computing with Python.\n",
        "- [matplotlib](http://matplotlib.org) : Library for plotting graphs in Python.\n",
        "- [RL-Glue](http://www.jmlr.org/papers/v10/tanner09a.html) : Library for reinforcement learning experiments.\n",
        "- [tqdm](https://tqdm.github.io/) : A package to display progress bar when running experiments.\n",
        "- BaseOptimizer : An abstract class that specifies the optimizer API for Agent.\n",
        "- plot_script : Custom script to plot results.\n",
        "- RandomWalkEnvironment : The Randomwalk environment script from Course 3 Assignment 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "82a5d2886220d5cf6288bc67624a8f9c",
          "grade": false,
          "grade_id": "cell-38bff794ab578cbf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "DUr78tVVG_Ca"
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell!\n",
        "\n",
        "# Import necessary libraries\n",
        "# DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os, shutil\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"Glues together an experiment, agent, and environment.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "class RLGlue:\n",
        "    \"\"\"RLGlue class\n",
        "\n",
        "    args:\n",
        "        env_name (string): the name of the module where the Environment class can be found\n",
        "        agent_name (string): the name of the module where the Agent class can be found\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_class, agent_class):\n",
        "        self.environment = env_class()\n",
        "        self.agent = agent_class()\n",
        "\n",
        "        self.total_reward = None\n",
        "        self.last_action = None\n",
        "        self.num_steps = None\n",
        "        self.num_episodes = None\n",
        "\n",
        "    def rl_init(self, agent_init_info={}, env_init_info={}):\n",
        "        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n",
        "        self.environment.env_init(env_init_info)\n",
        "        self.agent.agent_init(agent_init_info)\n",
        "\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 0\n",
        "        self.num_episodes = 0\n",
        "\n",
        "    def rl_start(self, agent_start_info={}, env_start_info={}):\n",
        "        \"\"\"Starts RLGlue experiment\n",
        "\n",
        "        Returns:\n",
        "            tuple: (state, action)\n",
        "        \"\"\"\n",
        "\n",
        "        last_state = self.environment.env_start()\n",
        "        self.last_action = self.agent.agent_start(last_state)\n",
        "\n",
        "        observation = (last_state, self.last_action)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def rl_agent_start(self, observation):\n",
        "        \"\"\"Starts the agent.\n",
        "\n",
        "        Args:\n",
        "            observation: The first observation from the environment\n",
        "\n",
        "        Returns:\n",
        "            The action taken by the agent.\n",
        "        \"\"\"\n",
        "        return self.agent.agent_start(observation)\n",
        "\n",
        "    def rl_agent_step(self, reward, observation):\n",
        "        \"\"\"Step taken by the agent\n",
        "\n",
        "        Args:\n",
        "            reward (float): the last reward the agent received for taking the\n",
        "                last action.\n",
        "            observation : the state observation the agent receives from the\n",
        "                environment.\n",
        "\n",
        "        Returns:\n",
        "            The action taken by the agent.\n",
        "        \"\"\"\n",
        "        return self.agent.agent_step(reward, observation)\n",
        "\n",
        "    def rl_agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates\n",
        "\n",
        "        Args:\n",
        "            reward (float): the reward the agent received when terminating\n",
        "        \"\"\"\n",
        "        self.agent.agent_end(reward)\n",
        "\n",
        "    def rl_env_start(self):\n",
        "        \"\"\"Starts RL-Glue environment.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): reward, state observation, boolean\n",
        "                indicating termination\n",
        "        \"\"\"\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 1\n",
        "\n",
        "        this_observation = self.environment.env_start()\n",
        "\n",
        "        return this_observation\n",
        "\n",
        "    def rl_env_step(self, action):\n",
        "        \"\"\"Step taken by the environment based on action from agent\n",
        "\n",
        "        Args:\n",
        "            action: Action taken by agent.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): reward, state observation, boolean\n",
        "                indicating termination.\n",
        "        \"\"\"\n",
        "        ro = self.environment.env_step(action)\n",
        "        (this_reward, _, terminal) = ro\n",
        "\n",
        "        self.total_reward += this_reward\n",
        "\n",
        "        if terminal:\n",
        "            self.num_episodes += 1\n",
        "        else:\n",
        "            self.num_steps += 1\n",
        "\n",
        "        return ro\n",
        "\n",
        "    def rl_step(self):\n",
        "        \"\"\"Step taken by RLGlue, takes environment step and either step or\n",
        "            end by agent.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, action, Boolean): reward, last state observation,\n",
        "                last action, boolean indicating termination\n",
        "        \"\"\"\n",
        "\n",
        "        (reward, last_state, term) = self.environment.env_step(self.last_action)\n",
        "\n",
        "        self.total_reward += reward;\n",
        "\n",
        "        if term:\n",
        "            self.num_episodes += 1\n",
        "            self.agent.agent_end(reward)\n",
        "            roat = (reward, last_state, None, term)\n",
        "        else:\n",
        "            self.num_steps += 1\n",
        "            self.last_action = self.agent.agent_step(reward, last_state)\n",
        "            roat = (reward, last_state, self.last_action, term)\n",
        "\n",
        "        return roat\n",
        "\n",
        "    def rl_cleanup(self):\n",
        "        \"\"\"Cleanup done at end of experiment.\"\"\"\n",
        "        self.environment.env_cleanup()\n",
        "        self.agent.agent_cleanup()\n",
        "\n",
        "    def rl_agent_message(self, message):\n",
        "        \"\"\"Message passed to communicate with agent during experiment\n",
        "\n",
        "        Args:\n",
        "            message: the message (or question) to send to the agent\n",
        "\n",
        "        Returns:\n",
        "            The message back (or answer) from the agent\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return self.agent.agent_message(message)\n",
        "\n",
        "    def rl_env_message(self, message):\n",
        "        \"\"\"Message passed to communicate with environment during experiment\n",
        "\n",
        "        Args:\n",
        "            message: the message (or question) to send to the environment\n",
        "\n",
        "        Returns:\n",
        "            The message back (or answer) from the environment\n",
        "\n",
        "        \"\"\"\n",
        "        return self.environment.env_message(message)\n",
        "\n",
        "    def rl_episode(self, max_steps_this_episode):\n",
        "        \"\"\"Runs an RLGlue episode\n",
        "\n",
        "        Args:\n",
        "            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n",
        "\n",
        "        Returns:\n",
        "            Boolean: if the episode should terminate\n",
        "        \"\"\"\n",
        "        is_terminal = False\n",
        "\n",
        "        self.rl_start()\n",
        "\n",
        "        while (not is_terminal) and ((max_steps_this_episode == 0) or\n",
        "                                     (self.num_steps < max_steps_this_episode)):\n",
        "            rl_step_result = self.rl_step()\n",
        "            is_terminal = rl_step_result[3]\n",
        "\n",
        "        return is_terminal\n",
        "\n",
        "    def rl_return(self):\n",
        "        \"\"\"The total reward\n",
        "\n",
        "        Returns:\n",
        "            float: the total reward\n",
        "        \"\"\"\n",
        "        return self.total_reward\n",
        "\n",
        "    def rl_num_steps(self):\n",
        "        \"\"\"The total number of steps taken\n",
        "\n",
        "        Returns:\n",
        "            Int: the total number of steps taken\n",
        "        \"\"\"\n",
        "        return self.num_steps\n",
        "\n",
        "    def rl_num_episodes(self):\n",
        "        \"\"\"The number of episodes\n",
        "\n",
        "        Returns\n",
        "            Int: the total number of episodes\n",
        "\n",
        "        \"\"\"\n",
        "        return self.num_episodes\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CarMJc_bHIgv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"Abstract environment base class for RL-Glue-py.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "\n",
        "class BaseEnvironment:\n",
        "    \"\"\"Implements the environment for an RLGlue environment\n",
        "\n",
        "    Note:\n",
        "        env_init, env_start, env_step, env_cleanup, and env_message are required\n",
        "        methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        reward = None\n",
        "        observation = None\n",
        "        termination = None\n",
        "        self.reward_obs_term = (reward, observation, termination)\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"Setup for the environment called when the experiment first starts.\n",
        "\n",
        "        Note:\n",
        "            Initialize a tuple with the reward, first state observation, boolean\n",
        "            indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_start(self):\n",
        "        \"\"\"The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_cleanup(self):\n",
        "        \"\"\"Cleanup done after the environment ends\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_message(self, message):\n",
        "        \"\"\"A message asking the environment for information\n",
        "\n",
        "        Args:\n",
        "            message: the message passed to the environment\n",
        "\n",
        "        Returns:\n",
        "            the response (or answer) to the message\n",
        "        \"\"\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QjTE3ybXHPnd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"An abstract class that specifies the Agent API for RL-Glue-py.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "\n",
        "class BaseAgent:\n",
        "    \"\"\"Implements the agent for an RL-Glue environment.\n",
        "    Note:\n",
        "        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n",
        "        agent_message are required methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_init(self, agent_info= {}):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_start(self, observation):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            observation (Numpy array): the state observation from the environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            observation (Numpy array): the state observation from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the terminal state.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_cleanup(self):\n",
        "        \"\"\"Cleanup done after the agent ends.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_message(self, message):\n",
        "        \"\"\"A function used to pass information from the agent to the experiment.\n",
        "        Args:\n",
        "            message: The message passed to the agent.\n",
        "        Returns:\n",
        "            The response (or answer) to the message.\n",
        "        \"\"\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jVTcDukaHWpr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\"\"\"An abstract class that specifies the optimizer API for Agent.\n",
        "\"\"\"\n",
        "\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "class BaseOptimizer:\n",
        "\t__metaclass__ = ABCMeta\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef optimizer_init(self, optimizer_info):\n",
        "\t\t\"\"\"Setup for the optimizer.\"\"\"\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef update_weights(self, weights, g):\n",
        "\t\t\"\"\"\n",
        "        Given weights and update g, return updated weights\n",
        "        \"\"\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "z6UCzrh2Hc9_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt1_legend_dict = {\"td_agent\": \"approximate values learned by\\n TD with neural network\",\n",
        "                    \"td_agent_5000_episodes\": \"approximate values learned by\\n TD with neural network\",\n",
        "                    \"td_agent_tilecoding\": \"approximate values learned by\\n TD with tile-coding\"}\n",
        "\n",
        "\n",
        "plt2_legend_dict = {\"td_agent\": \"TD with neural network\",\n",
        "                    \"td_agent_5000_episodes\": \"TD with neural network\",\n",
        "                    \"td_agent_tilecoding\": \"TD with tile-coding\"}\n",
        "\n",
        "\n",
        "plt2_label_dict = {\"td_agent\": \"RMSVE\\n averaged\\n over\\n 20 runs\",\n",
        "                   \"td_agent_5000_episodes\": \"RMSVE\\n averaged\\n over\\n 20 runs\",\n",
        "                   \"td_agent_tilecoding\": \"RMSVE\\n averaged\\n over\\n 20 runs\"}\n",
        "\n",
        "\n",
        "# Function to plot result\n",
        "def plot_result(data_name_array):\n",
        "\n",
        "    true_V = np.load('data/true_V.npy')\n",
        "\n",
        "    plt1_agent_sweeps = []\n",
        "    plt2_agent_sweeps = []\n",
        "\n",
        "    # two plots: learned state-value and learning curve (RMSVE)\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
        "\n",
        "    for data_name in data_name_array:\n",
        "\n",
        "        # plot1\n",
        "        filename = 'V_{}'.format(data_name).replace('.','')\n",
        "        current_agent_V = np.load('results/{}.npy'.format(filename))\n",
        "        current_agent_V = current_agent_V[-1, :]\n",
        "\n",
        "\n",
        "        plt1_x_legend = range(1,len(current_agent_V[:]) + 1)\n",
        "        graph_current_agent_V, = ax[0].plot(plt1_x_legend, current_agent_V[:], label=plt1_legend_dict[data_name])\n",
        "        plt1_agent_sweeps.append(graph_current_agent_V)\n",
        "\n",
        "        # plot2\n",
        "        filename = 'RMSVE_{}'.format(data_name).replace('.','')\n",
        "        RMSVE_data = np.load('results/{}.npz'.format(filename))\n",
        "        current_agent_RMSVE = np.mean(RMSVE_data[\"rmsve\"], axis = 0)\n",
        "\n",
        "        plt2_x_legend = np.arange(0, RMSVE_data[\"num_episodes\"]+1, RMSVE_data[\"eval_freq\"])\n",
        "        graph_current_agent_RMSVE, = ax[1].plot(plt2_x_legend, current_agent_RMSVE[:], label=plt2_legend_dict[data_name])\n",
        "        plt2_agent_sweeps.append(graph_current_agent_RMSVE)\n",
        "\n",
        "\n",
        "    # plot1:\n",
        "    # add True V\n",
        "    plt1_x_legend = range(1,len(true_V[:]) + 1)\n",
        "    graph_true_V, = ax[0].plot(plt1_x_legend, true_V[:], label=\"$v_{\\pi}$\")\n",
        "\n",
        "    ax[0].legend(handles=[*plt1_agent_sweeps, graph_true_V], fontsize = 13)\n",
        "\n",
        "    ax[0].set_title(\"State Value\", fontsize = 15)\n",
        "    ax[0].set_xlabel('State', fontsize = 14)\n",
        "    ax[0].set_ylabel('Value\\n scale', rotation=0, labelpad=15, fontsize = 14)\n",
        "\n",
        "    plt1_xticks = [1, 100, 200, 300, 400, 500]\n",
        "    plt1_yticks = [-1.0, 0.0, 1.0]\n",
        "    ax[0].set_xticks(plt1_xticks)\n",
        "    ax[0].set_xticklabels(plt1_xticks, fontsize=13)\n",
        "    ax[0].set_yticks(plt1_yticks)\n",
        "    ax[0].set_yticklabels(plt1_yticks, fontsize=13)\n",
        "\n",
        "\n",
        "    # plot2:\n",
        "    ax[1].legend(handles=plt2_agent_sweeps, fontsize = 13)\n",
        "\n",
        "    ax[1].set_title(\"Learning Curve\", fontsize = 15)\n",
        "    ax[1].set_xlabel('Episodes', fontsize = 14)\n",
        "    ax[1].set_ylabel(plt2_label_dict[data_name_array[0]], rotation=0, labelpad=40, fontsize = 14)\n",
        "\n",
        "    plt2_yticks = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "    ax[1].tick_params(axis=\"x\", labelsize=13)\n",
        "    ax[1].set_yticks(plt2_yticks)\n",
        "    ax[1].set_yticklabels(plt2_yticks, fontsize = 13)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CYF6eLkTHiZo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"RandomWalk environment class for RL-Glue-py.\n",
        "\"\"\"\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"Abstract environment base class for RL-Glue-py.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "\n",
        "class BaseEnvironment:\n",
        "    \"\"\"Implements the environment for an RLGlue environment\n",
        "\n",
        "    Note:\n",
        "        env_init, env_start, env_step, env_cleanup, and env_message are required\n",
        "        methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        reward = None\n",
        "        observation = None\n",
        "        termination = None\n",
        "        self.reward_obs_term = (reward, observation, termination)\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"Setup for the environment called when the experiment first starts.\n",
        "\n",
        "        Note:\n",
        "            Initialize a tuple with the reward, first state observation, boolean\n",
        "            indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_start(self):\n",
        "        \"\"\"The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_cleanup(self):\n",
        "        \"\"\"Cleanup done after the environment ends\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_message(self, message):\n",
        "        \"\"\"A message asking the environment for information\n",
        "\n",
        "        Args:\n",
        "            message: the message passed to the environment\n",
        "\n",
        "        Returns:\n",
        "            the response (or answer) to the message\n",
        "        \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class RandomWalkEnvironment(BaseEnvironment):\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"\n",
        "        Setup for the environment called when the experiment first starts.\n",
        "\n",
        "        Set parameters needed to setup the 500-state random walk environment.\n",
        "\n",
        "        Assume env_info dict contains:\n",
        "        {\n",
        "            num_states: 500,\n",
        "            start_state: 250,\n",
        "            left_terminal_state: 0,\n",
        "            right_terminal_state: 501,\n",
        "            seed: int\n",
        "        }\n",
        "        \"\"\"\n",
        "        # set random seed for each run\n",
        "        self.rand_generator = np.random.RandomState(env_info.get(\"seed\"))\n",
        "\n",
        "        ### Set each attributes correctly (4 lines)\n",
        "        # self.num_states = ?\n",
        "        # self.start_state = ?\n",
        "        # self.left_terminal_state = ?\n",
        "        # self.right_terminal_state = ?\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        self.num_states = env_info[\"num_states\"]\n",
        "        self.start_state = env_info[\"start_state\"]\n",
        "        self.left_terminal_state = env_info[\"left_terminal_state\"]\n",
        "        self.right_terminal_state = env_info[\"right_terminal_state\"]\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "    def env_start(self):\n",
        "        \"\"\"\n",
        "        The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "        ### set self.reward_obs_term tuple accordingly (3 lines)\n",
        "        # reward = ?\n",
        "        # observation = ?\n",
        "        # is_terminal = ?\n",
        "\n",
        "        ### START CODE HERE ### 3 lines\n",
        "\n",
        "\n",
        "        reward = 0.0\n",
        "        observation = self.start_state\n",
        "        is_terminal = False\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        self.reward_obs_term = (reward, observation, is_terminal)\n",
        "\n",
        "        # return first state observation from the environment\n",
        "        return self.reward_obs_term[1]\n",
        "\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "        ### set reward, current_state, and is_terminal correctly (10~12 lines)\n",
        "        # current state: next state after taking action from the last state [int]\n",
        "        # action: represents how many states to move from the last state [int]\n",
        "        # Hint: Given action (direction of movement), determine how much to move in that direction\n",
        "        #       by calling self.rand_generator.choice() once.\n",
        "        #       Solutions using other random methods may be graded as incorrect.\n",
        "        #       Remember all transitions beyond the terminal state is absorbed into the terminal state.\n",
        "        #\n",
        "        # reward = ?\n",
        "        # current_state = ?\n",
        "        # is_terminal = ?\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        last_state = self.reward_obs_term[1]\n",
        "\n",
        "        if action == 0: # left\n",
        "            current_state = max(self.left_terminal_state, last_state + self.rand_generator.choice(range(-100,0)))\n",
        "        elif action == 1: # right\n",
        "            current_state = min(self.right_terminal_state, last_state + self.rand_generator.choice(range(1,101)))\n",
        "        else:\n",
        "            raise ValueError(\"Wrong action value\")\n",
        "\n",
        "        reward = 0.0\n",
        "        is_terminal = False\n",
        "        if current_state == self.left_terminal_state:\n",
        "            reward = -1.0\n",
        "            is_terminal = True\n",
        "\n",
        "        elif current_state == self.right_terminal_state:\n",
        "            reward = 1.0\n",
        "            is_terminal = True\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        self.reward_obs_term = (reward, current_state, is_terminal)\n",
        "\n",
        "        return self.reward_obs_term"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B0TEby9RHoCs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "070cfb5d24dc4fb8f509f1dfac84a53e",
          "grade": false,
          "grade_id": "cell-78f0fcce11ea432d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "hnfWbWdBG_Ca"
      },
      "source": [
        "## Section 1: Create semi-gradient TD with a Neural Network\n",
        "In this section, you will implement an Agent that learns with semi-gradient TD with a neural network. You will use a neural network with one hidden layer. The input of the neural network is the one-hot encoding of the state number. We use the one-hot encoding of the state number instead of the state number itself because we do not want to build the prior knowledge that integer number inputs close to each other have similar values. The hidden layer contains 100 rectifier linear units (ReLUs) which pass their input if it is bigger than one and return 0 otherwise. ReLU gates are commonly used in neural networks due to their nice properties such as the sparsity of the activation and having non-vanishing gradients. The output of the neural network is the estimated state value. It is a linear function of the hidden units as is commonly the case when estimating the value of a continuous target using neural networks.\n",
        "\n",
        "The neural network looks like this:\n",
        "![](nn_structure.png)\n",
        "\n",
        "\n",
        "For a given input, $s$, value of $s$ is computed by:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\psi &= sW^{[0]} + b^{[0]} \\\\\n",
        "x &= \\textit{max}(0, \\psi) \\\\\n",
        "v &= xW^{[1]} + b^{[1]}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where $W^{[0]}$, $b^{[0]}$, $W^{[1]}$, $b^{[1]}$  are the parameters of the network and will be learned when training the agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "10552e26d4b34302228f5f65b1fbf110",
          "grade": false,
          "grade_id": "cell-9e85580f6dceec57",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "O0muMefaG_Cb"
      },
      "source": [
        "## 1-1: Implement helper methods\n",
        "\n",
        "Before implementing the agent, you first implement some helper functions which you will later use in agent's main methods.\n",
        "\n",
        "### Implement `get_value()`\n",
        "First, you will implement get_value() method which feeds an input $s$ into the neural network and returns the output of the network $v$ according to the equations above. To implement get_value(), take into account the following notes:\n",
        "\n",
        "- `get_value()` gets the one-hot encoded state number denoted by s as an input.\n",
        "- `get_value()` receives the weights of the neural network as input, denoted by weights and structured as an array of dictionaries. Each dictionary corresponds to weights from one layer of the neural network to the next. Each dictionary includes $W$ and $b$. The shape of the elements in weights are as follows:\n",
        "    - weights[0][\"W\"]: num_states $\\times$ num_hidden_units\n",
        "    - weights[0][\"b\"]: 1 $\\times$ num_hidden_units\n",
        "    - weights[1][\"W\"]: num_hidden_units $\\times$ 1\n",
        "    - weights[1][\"b\"]: 1 $\\times$ 1\n",
        "\n",
        "- The input of the neural network is a sparse vector. To make computation faster, we take advantage of input sparsity. To do so, we provided a helper method `my_matmul()`. **Make sure that you use `my_matmul()` for all matrix multiplications except for element-wise multiplications in this notebook.**\n",
        "- The max operator used for computing $x$ is element-wise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4f8ec434322d3ad6495ac5487be48e81",
          "grade": false,
          "grade_id": "cell-e6785c6caf24ec0b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_Mb-p7kpG_Cb"
      },
      "outputs": [],
      "source": [
        "def my_matmul(x1, x2):\n",
        "    \"\"\"\n",
        "    Given matrices x1 and x2, return the multiplication of them\n",
        "    \"\"\"\n",
        "\n",
        "    result = np.zeros((x1.shape[0], x2.shape[1]))\n",
        "    x1_non_zero_indices = x1.nonzero()\n",
        "    if x1.shape[0] == 1 and len(x1_non_zero_indices[1]) == 1:\n",
        "        result = x2[x1_non_zero_indices[1], :]\n",
        "    elif x1.shape[1] == 1 and len(x1_non_zero_indices[0]) == 1:\n",
        "        result[x1_non_zero_indices[0], :] = x2 * x1[x1_non_zero_indices[0], 0]\n",
        "    else:\n",
        "        result = np.matmul(x1, x2)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9e930c44dfc2c8afa24e19fec9e99b09",
          "grade": false,
          "grade_id": "cell-1cceb6da8f9e9a81",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "GJBJ_PXIG_Cb"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Graded Cell\n",
        "# -----------\n",
        "\n",
        "def get_value(s, weights):\n",
        "    \"\"\"\n",
        "    Compute value of input s given the weights of a neural network\n",
        "    \"\"\"\n",
        "    ### Compute the ouput of the neural network, v, for input s\n",
        "\n",
        "    # ----------------\n",
        "    # your code here\n",
        "    l1 = my_matmul(s, weights[0][\"W\"]) + weights[0][\"b\"]\n",
        "    l2 = np.maximum(0,l1)\n",
        "    v = my_matmul(l2, weights[1][\"W\"]) + weights[1][\"b\"]\n",
        "\n",
        "    # ----------------\n",
        "    return v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ff5fc3f71ba69cefe7a8703e631a8942",
          "grade": false,
          "grade_id": "cell-fb88de0206d4f8af",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "re0PWJmrG_Cb"
      },
      "source": [
        "Run the following code to test your implementation of the `get_value()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "98f912f3583e6053b9a9c63b307fc665",
          "grade": true,
          "grade_id": "cell-b6f5075598589be5",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "uyZEmZ4-G_Cb"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "# Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 10\n",
        "num_hidden_layer = 1\n",
        "s = np.array([[0, 0, 0, 1, 0]])\n",
        "\n",
        "weights_data = np.load(\"asserts/get_value_weights.npz\")\n",
        "weights = [dict() for i in range(num_hidden_layer+1)]\n",
        "weights[0][\"W\"] = weights_data[\"W0\"]\n",
        "weights[0][\"b\"] = weights_data[\"b0\"]\n",
        "weights[1][\"W\"] = weights_data[\"W1\"]\n",
        "weights[1][\"b\"] = weights_data[\"b1\"]\n",
        "\n",
        "estimated_value = get_value(s, weights)\n",
        "print (\"Estimated value: {}\".format(estimated_value))\n",
        "\n",
        "assert(np.allclose(estimated_value, [[-0.21915705]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7bb9e879b94a52f6af662b6ef3c3303b",
          "grade": false,
          "grade_id": "cell-64b2f1df24b85918",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "WtelhckvG_Cb"
      },
      "source": [
        "**Expected output**:\n",
        "\n",
        "    Estimated value: [[-0.21915705]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "75eabe0413eca786bbbb495b82b0c787",
          "grade": false,
          "grade_id": "cell-f756ee63a7642be4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "RXEO_k0FG_Cc"
      },
      "source": [
        "### Implement `get_gradient()`\n",
        "You will also implement `get_gradient()` method which computes the gradient of the value function for a given input, using backpropagation. You will later use this function to update the value function.\n",
        "\n",
        "As you know, we compute the value of a state $s$ according to:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\psi &= sW^{[0]} + b^{[0]} \\\\\n",
        "x &= \\textit{max}(0, \\psi) \\\\\n",
        "v &= xW^{[1]} + b^{[1]}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "To update the weights of the neural network ($W^{[0]}$, $b^{[0]}$, $W^{[1]}$, $b^{[1]}$), we compute the gradient of $v$ with respect to the weights according to:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial v}{\\partial W^{[0]}} &= s^T(W^{[1]T} \\odot I_{x>0}) \\\\\n",
        "\\frac{\\partial v}{\\partial b^{[0]}} &= W^{[1]T} \\odot I_{x>0} \\\\\n",
        "\\frac{\\partial v}{\\partial W^{[1]}} &= x^T \\\\\n",
        "\\frac{\\partial v}{\\partial b^{[1]}} &= 1\n",
        "\\end{align}\n",
        "$$\n",
        "where $\\odot$ denotes element-wise matrix multiplication and $I_{x>0}$ is the gradient of the ReLU activation function which is an indicator whose $i$th element is 1 if $x[i]>0$ and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ec551777bb2afb289bef59a6dcd99d01",
          "grade": false,
          "grade_id": "cell-7c2e341b5e17073f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "qKuqIra3G_Cc"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Graded Cell\n",
        "# -----------\n",
        "\n",
        "def get_gradient(s, weights):\n",
        "    \"\"\"\n",
        "    Given inputs s and weights, return the gradient of v with respect to the weights\n",
        "    \"\"\"\n",
        "\n",
        "    ### Compute the gradient of the value function with respect to W0, b0, W1, b1 for input s\n",
        "    # grads[0][\"W\"] = ?\n",
        "    # grads[0][\"b\"] = ?\n",
        "    # grads[1][\"W\"] = ?\n",
        "    # grads[1][\"b\"] = ?\n",
        "    # Note that grads[0][\"W\"], grads[0][\"b\"], grads[1][\"W\"], and grads[1][\"b\"] should have the same shape as\n",
        "    # weights[0][\"W\"], weights[0][\"b\"], weights[1][\"W\"], and weights[1][\"b\"] respectively\n",
        "    # Note that to compute the gradients, you need to compute the activation of the hidden layer (x)\n",
        "\n",
        "    grads = [dict() for i in range(len(weights))]\n",
        "\n",
        "    # ----------------\n",
        "    # your code here\n",
        "    # y = (relu(s * w1 + b1) * w2 + b2)\n",
        "    l1 = my_matmul(s, weights[0][\"W\"]) + weights[0][\"b\"] #(s * w1 + b1)\n",
        "    l2 = np.maximum(0,l1) #relu(s * w1 + b1)\n",
        "\n",
        "\n",
        "    #b2  dY/db2 = 1 :\n",
        "    grads[1][\"b\"] = [[1.]]\n",
        "    #print(\"b2\", grads[1][\"b\"])\n",
        "    #w2 dy/dw2 = relu(s * w1 + b1):\n",
        "    grads[1][\"W\"] = l2.T\n",
        "    #print(\"w2\", grads[1][\"W\"])\n",
        "    #b1: dy/db1 = (dy/d relu(s * w1 + b1)) * (d relu(s * w1 + b1)/db1)\n",
        "    d_b1 = np.array(weights[1][\"W\"], copy = True)\n",
        "    d_b1[l2<=0] = 0\n",
        "    grads[0][\"b\"] = d_b1.T #(weights[1][\"W\"] * (np.where(l1 > 0, 1, 0)))[:,0]\n",
        "    #print(\"b1\", grads[0][\"b\"])\n",
        "    #w1:  dy/dw1 = (dy/d relu(s * w1 + b1)) * (d relu(s * w1 + b1)/dw1)\n",
        "    grads[0][\"W\"] = (my_matmul(d_b1 , s)).T\n",
        "\n",
        "    #print(\"w1\",grads[0][\"W\"])\n",
        "\n",
        "\n",
        "    # ----------------\n",
        "\n",
        "    return grads\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2d63e7d358a94c10e1d6abe8e86e78ae",
          "grade": false,
          "grade_id": "cell-c7174ebafb9fd262",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "dL-mo7JkG_Cc"
      },
      "source": [
        "Run the following code to test your implementation of the `get_gradient()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4f6cb28ed1eb7f24dbb49181cde1030c",
          "grade": true,
          "grade_id": "cell-857ce864dc0a98bc",
          "locked": true,
          "points": 40,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "VXlcIWvAG_Cc"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "# Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 2\n",
        "num_hidden_layer = 1\n",
        "s = np.array([[0, 0, 0, 1, 0]])\n",
        "\n",
        "weights_data = np.load(\"asserts/get_gradient_weights.npz\")\n",
        "weights = [dict() for i in range(num_hidden_layer+1)]\n",
        "weights[0][\"W\"] = weights_data[\"W0\"]\n",
        "weights[0][\"b\"] = weights_data[\"b0\"]\n",
        "weights[1][\"W\"] = weights_data[\"W1\"]\n",
        "weights[1][\"b\"] = weights_data[\"b1\"]\n",
        "\n",
        "grads = get_gradient(s, weights)\n",
        "\n",
        "grads_answer = np.load(\"asserts/get_gradient_grads.npz\")\n",
        "\n",
        "assert(np.allclose(grads[0][\"W\"], grads_answer[\"W0\"]))\n",
        "assert(np.allclose(grads[0][\"b\"], grads_answer[\"b0\"]))\n",
        "assert(np.allclose(grads[1][\"W\"], grads_answer[\"W1\"]))\n",
        "assert(np.allclose(grads[1][\"b\"], grads_answer[\"b1\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0cd83a924c5f2539d8667f904ba9dc1e",
          "grade": false,
          "grade_id": "cell-c0b34db819e33427",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "TvOyhZNsG_Cc"
      },
      "source": [
        "**Expected output**:\n",
        "\n",
        "    grads[0][\"W\"]\n",
        "     [[0.         0.        ]\n",
        "     [0.         0.        ]\n",
        "     [0.         0.        ]\n",
        "     [0.76103773 0.12167502]\n",
        "     [0.         0.        ]]\n",
        "\n",
        "    grads[0][\"b\"]\n",
        "     [[0.76103773 0.12167502]]\n",
        "\n",
        "    grads[1][\"W\"]\n",
        "     [[0.69198983]\n",
        "     [0.82403662]]\n",
        "\n",
        "    grads[1][\"b\"]\n",
        "     [[1.]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "b452cba832e14bdaafaef6e2325094f8",
          "grade": false,
          "grade_id": "cell-5643ccac3b1269a7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "GqPNbVq8G_Cc"
      },
      "source": [
        "### Implement stochastic gradient descent method for state-value prediction\n",
        "In this section, you will implement stochastic gradient descent (SGD) method for state_value prediction. Here is the basic SGD update for state-value prediction with TD:\n",
        "\n",
        "$$\\mathbf{w_{t+1}} = \\mathbf{w_{t}} + \\alpha \\delta_t \\nabla \\hat{v}(S_t,\\mathbf{w_{t}})$$\n",
        "\n",
        "At each time step, we update the weights in the direction  $g_t = \\delta_t \\nabla \\hat{v}(S_t,\\mathbf{w_t})$ using a fixed step-size $\\alpha$. $\\delta_t = R_{t+1} + \\gamma \\hat{v}(S_{t+1},\\mathbf{w_{t}}) - \\hat{v}(S_t,\\mathbf{w_t})$ is the TD-error. $\\nabla \\hat{v}(S_t,\\mathbf{w_{t}})$ is the gradient of the value function with respect to the weights.\n",
        "\n",
        "The following cell includes the SGD class. You will complete the `update_weight()` method of SGD assuming that the weights and update g are provided.\n",
        "\n",
        "**As you know, in this assignment, we structured the weights as an array of dictionaries. Note that the updates $g_t$, in the case of TD, is $\\delta_t \\nabla \\hat{v}(S_t,\\mathbf{w_t})$. As a result, $g_t$ has the same structure as $\\nabla \\hat{v}(S_t,\\mathbf{w_t})$ which is also an array of dictionaries.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1a60a3c6222a9723c128919b71f5f5fd",
          "grade": false,
          "grade_id": "cell-6e90dc8c0b50f536",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "HqTZVxn4G_Cc"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Graded Cell\n",
        "# -----------\n",
        "\n",
        "class SGD(BaseOptimizer):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def optimizer_init(self, optimizer_info):\n",
        "        \"\"\"Setup for the optimizer.\n",
        "\n",
        "        Set parameters needed to setup the stochastic gradient descent method.\n",
        "\n",
        "        Assume optimizer_info dict contains:\n",
        "        {\n",
        "            step_size: float\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.step_size = optimizer_info.get(\"step_size\")\n",
        "\n",
        "    def update_weights(self, weights, g):\n",
        "        \"\"\"\n",
        "        Given weights and update g, return updated weights\n",
        "        \"\"\"\n",
        "        for i in range(len(weights)):\n",
        "            for param in weights[i].keys():\n",
        "\n",
        "                ### update weights\n",
        "                # weights[i][param] = None\n",
        "\n",
        "                # ----------------\n",
        "                # your code here\n",
        "                #+1= +  * (+1+ (+1,) (,)) *  (,)\n",
        "\n",
        "                weights[i][param] = weights[i][param] + self.step_size * g[i][param]\n",
        "                # ----------------\n",
        "\n",
        "        return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "596f56e4daf04d33a63d763ac43939a4",
          "grade": false,
          "grade_id": "cell-2b1d9e89df9a0960",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "CEFiWOlmG_Cd"
      },
      "source": [
        "Run the following code to test your implementation of the `update_weights()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9869582597f7335dd078b4a9fc16f83a",
          "grade": true,
          "grade_id": "cell-474daca9520b5361",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "eMKN2Nx3G_Cd"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "# Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 2\n",
        "num_hidden_layer = 1\n",
        "\n",
        "weights_data = np.load(\"asserts/update_weights_weights.npz\")\n",
        "weights = [dict() for i in range(num_hidden_layer+1)]\n",
        "weights[0][\"W\"] = weights_data[\"W0\"]\n",
        "weights[0][\"b\"] = weights_data[\"b0\"]\n",
        "weights[1][\"W\"] = weights_data[\"W1\"]\n",
        "weights[1][\"b\"] = weights_data[\"b1\"]\n",
        "\n",
        "g_data = np.load(\"asserts/update_weights_g.npz\")\n",
        "g = [dict() for i in range(num_hidden_layer+1)]\n",
        "g[0][\"W\"] = g_data[\"W0\"]\n",
        "g[0][\"b\"] = g_data[\"b0\"]\n",
        "g[1][\"W\"] = g_data[\"W1\"]\n",
        "g[1][\"b\"] = g_data[\"b1\"]\n",
        "\n",
        "test_sgd = SGD()\n",
        "optimizer_info = {\"step_size\": 0.3}\n",
        "test_sgd.optimizer_init(optimizer_info)\n",
        "updated_weights = test_sgd.update_weights(weights, g)\n",
        "\n",
        "# updated weights asserts\n",
        "updated_weights_answer = np.load(\"asserts/update_weights_updated_weights.npz\")\n",
        "\n",
        "assert(np.allclose(updated_weights[0][\"W\"], updated_weights_answer[\"W0\"]))\n",
        "assert(np.allclose(updated_weights[0][\"b\"], updated_weights_answer[\"b0\"]))\n",
        "assert(np.allclose(updated_weights[1][\"W\"], updated_weights_answer[\"W1\"]))\n",
        "assert(np.allclose(updated_weights[1][\"b\"], updated_weights_answer[\"b1\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "841b34303c37226c8d615781746b441f",
          "grade": false,
          "grade_id": "cell-a455deb7a6201e7b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "LgYOgxMPG_Cd"
      },
      "source": [
        "**Expected output**:\n",
        "\n",
        "    updated_weights[0][\"W\"]\n",
        "     [[ 1.17899492  0.53656321]\n",
        "     [ 0.58008221  1.47666572]\n",
        "     [ 1.01909411 -1.10248056]\n",
        "     [ 0.72490408  0.06828853]\n",
        "     [-0.20609725  0.69034095]]\n",
        "\n",
        "    updated_weights[0][\"b\"]\n",
        "     [[-0.18484533  0.92844539]]\n",
        "\n",
        "    updated_weights[1][\"W\"]\n",
        "     [[0.70488257]\n",
        "     [0.58150878]]\n",
        "\n",
        "    updated_weights[1][\"b\"]\n",
        "     [[0.88467086]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bdbb448de97e2ceb2da940e6f2bc475d",
          "grade": false,
          "grade_id": "cell-abffd08f353110c9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "tIn4i2-sG_Cd"
      },
      "source": [
        "### Adam Algorithm\n",
        "In this assignment, instead of using SGD for updating the weights, we use a more advanced algorithm called Adam. The Adam algorithm improves the SGD update with two concepts: adaptive vector step-sizes and momentum. It keeps estimates of the mean and second moment of the updates, denoted by $\\mathbf{m}$ and $\\mathbf{v}$ respectively:\n",
        "$$\\mathbf{m_t} = \\beta_m \\mathbf{m_{t-1}} + (1 - \\beta_m)g_t \\\\\n",
        "\\mathbf{v_t} = \\beta_v \\mathbf{v_{t-1}} + (1 - \\beta_v)g^2_t\n",
        "$$\n",
        "\n",
        "Given that $\\mathbf{m}$ and $\\mathbf{v}$ are initialized to zero, they are biased toward zero. To get unbiased estimates of the mean and second moment, Adam defines $\\mathbf{\\hat{m}}$ and $\\mathbf{\\hat{v}}$ as:\n",
        "$$ \\mathbf{\\hat{m_t}} = \\frac{\\mathbf{m_t}}{1 - \\beta_m^t} \\\\\n",
        "\\mathbf{\\hat{v_t}} = \\frac{\\mathbf{v_t}}{1 - \\beta_v^t}\n",
        "$$\n",
        "\n",
        "The weights are then updated as follows:\n",
        "$$ \\mathbf{w_t} = \\mathbf{w_{t-1}} + \\frac{\\alpha}{\\sqrt{\\mathbf{\\hat{v_t}}}+\\epsilon} \\mathbf{\\hat{m_t}}\n",
        "$$\n",
        "\n",
        "When implementing the agent you will use the Adam algorithm instead of SGD because it is more efficient. We have already provided you the implementation of the Adam algorithm in the cell below. You will use it when implementing your agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "693d46afb36061fa97532bd63656a034",
          "grade": false,
          "grade_id": "cell-d5eb6f0601a69b03",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "bBNFXQsiG_Cd"
      },
      "outputs": [],
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "class Adam(BaseOptimizer):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def optimizer_init(self, optimizer_info):\n",
        "        \"\"\"Setup for the optimizer.\n",
        "\n",
        "        Set parameters needed to setup the Adam algorithm.\n",
        "\n",
        "        Assume optimizer_info dict contains:\n",
        "        {\n",
        "            num_states: integer,\n",
        "            num_hidden_layer: integer,\n",
        "            num_hidden_units: integer,\n",
        "            step_size: float,\n",
        "            self.beta_m: float\n",
        "            self.beta_v: float\n",
        "            self.epsilon: float\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_states = optimizer_info.get(\"num_states\")\n",
        "        self.num_hidden_layer = optimizer_info.get(\"num_hidden_layer\")\n",
        "        self.num_hidden_units = optimizer_info.get(\"num_hidden_units\")\n",
        "\n",
        "        # Specify Adam algorithm's hyper parameters\n",
        "        self.step_size = optimizer_info.get(\"step_size\")\n",
        "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
        "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
        "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
        "\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
        "\n",
        "        # Initialize Adam algorithm's m and v\n",
        "        self.m = [dict() for i in range(self.num_hidden_layer+1)]\n",
        "        self.v = [dict() for i in range(self.num_hidden_layer+1)]\n",
        "\n",
        "        for i in range(self.num_hidden_layer+1):\n",
        "\n",
        "            # Initialize self.m[i][\"W\"], self.m[i][\"b\"], self.v[i][\"W\"], self.v[i][\"b\"] to zero\n",
        "            self.m[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i+1]))\n",
        "            self.m[i][\"b\"] = np.zeros((1, self.layer_size[i+1]))\n",
        "            self.v[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i+1]))\n",
        "            self.v[i][\"b\"] = np.zeros((1, self.layer_size[i+1]))\n",
        "\n",
        "        # Initialize beta_m_product and beta_v_product to be later used for computing m_hat and v_hat\n",
        "        self.beta_m_product = self.beta_m\n",
        "        self.beta_v_product = self.beta_v\n",
        "\n",
        "    def update_weights(self, weights, g):\n",
        "        \"\"\"\n",
        "        Given weights and update g, return updated weights\n",
        "        \"\"\"\n",
        "\n",
        "        for i in range(len(weights)):\n",
        "            for param in weights[i].keys():\n",
        "\n",
        "                ### update self.m and self.v\n",
        "                self.m[i][param] = self.beta_m * self.m[i][param] + (1 - self.beta_m) * g[i][param]\n",
        "                self.v[i][param] = self.beta_v * self.v[i][param] + (1 - self.beta_v) * (g[i][param] * g[i][param])\n",
        "\n",
        "                ### compute m_hat and v_hat\n",
        "                m_hat = self.m[i][param] / (1 - self.beta_m_product)\n",
        "                v_hat = self.v[i][param] / (1 - self.beta_v_product)\n",
        "\n",
        "                ### update weights\n",
        "                weights[i][param] += self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "        ### update self.beta_m_product and self.beta_v_product\n",
        "        self.beta_m_product *= self.beta_m\n",
        "        self.beta_v_product *= self.beta_v\n",
        "\n",
        "        return weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "70c99058737a1cc90c08a5bb3dfa094f",
          "grade": false,
          "grade_id": "cell-8e93d33d8b0d7ed2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "JaK1ry-4G_Cd"
      },
      "source": [
        "## 1-2: Implement Agent Methods\n",
        "In this section, you will implement `agent_init()`, `agent_start()`, `agent_step()`, and `agent_end()`.\n",
        "\n",
        "In `agent_init()`, you will:\n",
        "   \n",
        "   - specify the neural network structure by filling self.layer_size with the size of the input layer, hidden layer, and output layer.\n",
        "   - initialize the network's parameters. We show the parameters as an array of dictionaries, self.weights, where each dictionary corresponds to weights from one layer to the next. Each dictionary includes $W$ and $b$. To initialize the parameters, you will use a normal distribution with mean 0 and standard deviation $\\sqrt{\\frac{2}{\\text{# input of each node}}}$. This initialization heuristic is commonly used when using ReLU gates and helps keep the output of a neuron from getting too big or too small. To initialize the network's parameters, use **self.rand_generator.normal()** which draws random samples from a normal distribution. The parameters of self.rand_generator.normal are mean of the distribution, standard deviation of the distribution, and output shape in the form of tuple of integers.\n",
        "\n",
        "\n",
        "In `agent_start()`, you will:\n",
        "   - specify self.last_state and self.last_action.\n",
        "   \n",
        "In `agent_step()` and `agent_end()`, you will:\n",
        "   - compute the TD error using $v(S_t)$ and $v(S_{t+1})$. To compute the value function for $S_t$ and $S_{t+1}$, you will get their one-hot encoding using `one_hot()` method that we provided below. You feed the one-hot encoded state number to the neural networks using `get_value()` method that you implemented above. Note that `one_hot()` method returns the one-hot encoding of a state as a numpy array of shape (1, num_states).\n",
        "   - retrieve the gradients using `get_gradient()` function that you implemented.\n",
        "   - use Adam_algorithm that we provided to update the neural network's parameters, self.weights.\n",
        "   - use `agent_policy()` method to select actions with. (only in `agent_step()`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fd3b6cf36205312bb4d46b50064433c6",
          "grade": false,
          "grade_id": "cell-513bae9622f7055c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "848MuJ7vG_Cd"
      },
      "outputs": [],
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "def one_hot(state, num_states):\n",
        "    \"\"\"\n",
        "    Given num_state and a state, return the one-hot encoding of the state\n",
        "    \"\"\"\n",
        "    # Create the one-hot encoding of state\n",
        "    # one_hot_vector is a numpy array of shape (1, num_states)\n",
        "\n",
        "    one_hot_vector = np.zeros((1, num_states))\n",
        "    one_hot_vector[0, int((state - 1))] = 1\n",
        "\n",
        "    return one_hot_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "36476121667837625334e9ec711b1840",
          "grade": false,
          "grade_id": "cell-23b7497bda6c8936",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "MaweOULbG_Cd"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Graded Cell\n",
        "# -----------\n",
        "\n",
        "class TDAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        self.name = \"td_agent\"\n",
        "        pass\n",
        "\n",
        "    def agent_init(self, agent_info={}):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\n",
        "\n",
        "        Set parameters needed to setup the semi-gradient TD with a Neural Network.\n",
        "\n",
        "        Assume agent_info dict contains:\n",
        "        {\n",
        "            num_states: integer,\n",
        "            num_hidden_layer: integer,\n",
        "            num_hidden_units: integer,\n",
        "            step_size: float,\n",
        "            discount_factor: float,\n",
        "            self.beta_m: float\n",
        "            self.beta_v: float\n",
        "            self.epsilon: float\n",
        "            seed: int\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        # Set random seed for weights initialization for each run\n",
        "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n",
        "\n",
        "        # Set random seed for policy for each run\n",
        "        self.policy_rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n",
        "\n",
        "        # Set attributes according to agent_info\n",
        "        self.num_states = agent_info.get(\"num_states\")\n",
        "        self.num_hidden_layer = agent_info.get(\"num_hidden_layer\")\n",
        "        self.num_hidden_units = agent_info.get(\"num_hidden_units\")\n",
        "        self.discount_factor = agent_info.get(\"discount_factor\")\n",
        "\n",
        "        ### Define the neural network's structure\n",
        "        # Specify self.layer_size which shows the number of nodes in each layer\n",
        "        # self.layer_size = np.array([None, None, None])\n",
        "        # Hint: Checkout the NN diagram at the beginning of the notebook\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, self.num_hidden_layer])\n",
        "        # ----------------\n",
        "\n",
        "        # Initialize the neural network's parameter\n",
        "        self.weights = [dict() for i in range(self.num_hidden_layer+1)]\n",
        "        std = np.sqrt(2 / self.num_states)\n",
        "        for i in range(self.num_hidden_layer+1):\n",
        "            #print(\"i\",i)\n",
        "            ### Initialize self.weights[i][\"W\"] and self.weights[i][\"b\"] using self.rand_generator.normal()\n",
        "            # Note that The parameters of self.rand_generator.normal are mean of the distribution,\n",
        "            # standard deviation of the distribution, and output shape in the form of tuple of integers.\n",
        "            # To specify output shape, use self.layer_size.\n",
        "\n",
        "            # ----------------\n",
        "            # your code here\n",
        "\n",
        "            input_size, output_size = self.layer_size[i], self.layer_size[i+1]\n",
        "            self.weights[i][\"W\"] = self.rand_generator.normal(0, np.sqrt(2/input_size), (input_size, output_size))\n",
        "            self.weights[i][\"b\"] = self.rand_generator.normal(0, np.sqrt(2/input_size), (1, output_size))\n",
        "\n",
        "            # ----------------\n",
        "\n",
        "        # Specify the optimizer\n",
        "        self.optimizer = Adam()\n",
        "        self.optimizer.optimizer_init({\n",
        "            \"num_states\": agent_info[\"num_states\"],\n",
        "            \"num_hidden_layer\": agent_info[\"num_hidden_layer\"],\n",
        "            \"num_hidden_units\": agent_info[\"num_hidden_units\"],\n",
        "            \"step_size\": agent_info[\"step_size\"],\n",
        "            \"beta_m\": agent_info[\"beta_m\"],\n",
        "            \"beta_v\": agent_info[\"beta_v\"],\n",
        "            \"epsilon\": agent_info[\"epsilon\"],\n",
        "        })\n",
        "\n",
        "        self.last_state = None\n",
        "        self.last_action = None\n",
        "\n",
        "    def agent_policy(self, state):\n",
        "\n",
        "        ### Set chosen_action as 0 or 1 with equal probability.\n",
        "        chosen_action = self.policy_rand_generator.choice([0,1])\n",
        "        return chosen_action\n",
        "\n",
        "    def agent_start(self, state):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            state (Numpy array): the state from the\n",
        "                environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "        ### select action given state (using self.agent_policy()), and save current state and action\n",
        "        # self.last_state = ?\n",
        "        # self.last_action = ?\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        a = self.agent_policy(state)\n",
        "        self.last_state = state\n",
        "        self.last_action = a\n",
        "        # ----------------\n",
        "\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, state):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            state (Numpy array): the state from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "\n",
        "        ### Compute TD error\n",
        "        # delta = None\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        s_old = one_hot(self.last_state, self.num_states)\n",
        "        old_pred = get_value(s_old, self.weights)\n",
        "        s = one_hot(state, self.num_states)\n",
        "        y_pred = get_value(s, self.weights)\n",
        "        delta = (reward + self.discount_factor * y_pred - old_pred)\n",
        "        # ----------------\n",
        "\n",
        "        ### Retrieve gradients\n",
        "        # grads = None\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        grads = get_gradient(s_old, self.weights)\n",
        "        #print(\"grads\", grads)\n",
        "        # ----------------\n",
        "\n",
        "        ### Compute g (1 line)\n",
        "        g = [dict() for i in range(self.num_hidden_layer+1)]\n",
        "        #print(\"g\",g)\n",
        "        for i in range(self.num_hidden_layer+1):\n",
        "            for param in self.weights[i].keys():\n",
        "\n",
        "                # g[i][param] = None\n",
        "                # ----------------\n",
        "                # your code here\n",
        "                #g = (+1+ (+1,) (,)) * grads\n",
        "                #print(f\"key {i} {param}\")\n",
        "                g[i][param] = delta * grads[i][param]\n",
        "                # ----------------\n",
        "\n",
        "        ### update the weights using self.optimizer\n",
        "        # self.weights = None\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        #print(\"old weights\", self.weights)\n",
        "        self.weights = self.optimizer.update_weights(self.weights, g)\n",
        "        #print(\"new weights\", self.weights)\n",
        "        # ----------------\n",
        "\n",
        "        ### update self.last_state and self.last_action\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        a = self.agent_policy(state)\n",
        "        self.last_state = state\n",
        "        self.last_action = a\n",
        "        # ----------------\n",
        "\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the\n",
        "                terminal state.\n",
        "        \"\"\"\n",
        "\n",
        "        ### compute TD error\n",
        "        # delta = None\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        last_state_vector = one_hot(self.last_state, self.num_states)\n",
        "        last_value = get_value(last_state_vector, self.weights)\n",
        "\n",
        "        delta = reward - last_value\n",
        "        # ----------------\n",
        "\n",
        "        ### Retrieve gradients\n",
        "        # grads = None\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        grads = get_gradient(last_state_vector, self.weights)\n",
        "        # ----------------\n",
        "\n",
        "        ### Compute g\n",
        "        g = [dict() for i in range(self.num_hidden_layer+1)]\n",
        "        for i in range(self.num_hidden_layer+1):\n",
        "            for param in self.weights[i].keys():\n",
        "                pass\n",
        "                # g[i][param] = None\n",
        "                # ----------------\n",
        "                # your code here\n",
        "                g[i][param] = delta * grads[i][param]\n",
        "                # ----------------\n",
        "\n",
        "        ### update the weights using self.optimizer\n",
        "        # self.weights = None\n",
        "\n",
        "        # ----------------\n",
        "        # your code here\n",
        "        self.weights = self.optimizer.update_weights(self.weights, g)\n",
        "        # ----------------\n",
        "\n",
        "    def agent_message(self, message):\n",
        "        if message == 'get state value':\n",
        "            state_value = np.zeros(self.num_states)\n",
        "            for state in range(1, self.num_states + 1):\n",
        "                s = one_hot(state, self.num_states)\n",
        "                state_value[state - 1] = get_value(s, self.weights)\n",
        "            return state_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d3453889decc8a04162d98d7498f55c7",
          "grade": false,
          "grade_id": "cell-983e76457252cdfb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "TScU6EaKG_Ce"
      },
      "source": [
        "Run the following code to test your implementation of the `agent_init()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4e3141598dfbac61e77d655b6dad2b92",
          "grade": true,
          "grade_id": "cell-f0743327afc7dc60",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Hl1yKYrfG_Ce"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "agent_info = {\n",
        "    \"num_states\": 5,\n",
        "    \"num_hidden_layer\": 1,\n",
        "    \"num_hidden_units\": 2,\n",
        "    \"step_size\": 0.25,\n",
        "    \"discount_factor\": 0.9,\n",
        "    \"beta_m\": 0.9,\n",
        "    \"beta_v\": 0.99,\n",
        "    \"epsilon\": 0.0001,\n",
        "    \"seed\": 0,\n",
        "}\n",
        "\n",
        "test_agent = TDAgent()\n",
        "test_agent.agent_init(agent_info)\n",
        "\n",
        "print(\"layer_size: {}\".format(test_agent.layer_size))\n",
        "assert(np.allclose(test_agent.layer_size, np.array([agent_info[\"num_states\"],\n",
        "                                                    agent_info[\"num_hidden_units\"],\n",
        "                                                    1])))\n",
        "\n",
        "assert(test_agent.weights[0][\"W\"].shape == (agent_info[\"num_states\"], agent_info[\"num_hidden_units\"]))\n",
        "assert(test_agent.weights[0][\"b\"].shape == (1, agent_info[\"num_hidden_units\"]))\n",
        "assert(test_agent.weights[1][\"W\"].shape == (agent_info[\"num_hidden_units\"], 1))\n",
        "assert(test_agent.weights[1][\"b\"].shape == (1, 1))\n",
        "\n",
        "agent_weight_answer = np.load(\"asserts/agent_init_weights_1.npz\")\n",
        "assert(np.allclose(test_agent.weights[0][\"W\"], agent_weight_answer[\"W0\"]))\n",
        "assert(np.allclose(test_agent.weights[0][\"b\"], agent_weight_answer[\"b0\"]))\n",
        "assert(np.allclose(test_agent.weights[1][\"W\"], agent_weight_answer[\"W1\"]))\n",
        "assert(np.allclose(test_agent.weights[1][\"b\"], agent_weight_answer[\"b1\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6302330cea7055e459017cb987693216",
          "grade": false,
          "grade_id": "cell-8c58d16039ba70f3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "jTrgozNWG_Ce"
      },
      "source": [
        "**Expected output**:\n",
        "\n",
        "    layer_size: [5 2 1]\n",
        "    weights[0][\"W\"] shape: (5, 2)\n",
        "    weights[0][\"b\"] shape: (1, 2)\n",
        "    weights[1][\"W\"] shape: (2, 1)\n",
        "    weights[1][\"b\"] shape: (1, 1)\n",
        "\n",
        "    weights[0][\"W\"]\n",
        "     [[ 1.11568467  0.25308164]\n",
        "     [ 0.61900825  1.4172653 ]\n",
        "     [ 1.18114738 -0.6180848 ]\n",
        "     [ 0.60088868 -0.0957267 ]\n",
        "     [-0.06528133  0.25968529]]\n",
        "\n",
        "    weights[0][\"b\"]\n",
        "     [[0.09110115 0.91976332]]\n",
        "\n",
        "    weights[1][\"W\"]\n",
        "     [[0.76103773]\n",
        "     [0.12167502]]\n",
        "\n",
        "    weights[1][\"b\"]\n",
        "     [[0.44386323]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "84b73bc9f975e1856e55e1653045d7b0",
          "grade": false,
          "grade_id": "cell-2b1d685389133b08",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "PlKtBQbWG_Ce"
      },
      "source": [
        "Run the following code to test your implementation of the `agent_start()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3be7ae1a33486ecc2b773603d11a050f",
          "grade": true,
          "grade_id": "cell-ceb93952a916aa5a",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3JYNioPuG_Ce"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "agent_info = {\n",
        "    \"num_states\": 500,\n",
        "    \"num_hidden_layer\": 1,\n",
        "    \"num_hidden_units\": 100,\n",
        "    \"step_size\": 0.1,\n",
        "    \"discount_factor\": 1.0,\n",
        "    \"beta_m\": 0.9,\n",
        "    \"beta_v\": 0.99,\n",
        "    \"epsilon\": 0.0001,\n",
        "    \"seed\": 10,\n",
        "}\n",
        "\n",
        "# Suppose state = 250\n",
        "state = 250\n",
        "\n",
        "test_agent = TDAgent()\n",
        "test_agent.agent_init(agent_info)\n",
        "test_agent.agent_start(state)\n",
        "\n",
        "assert(test_agent.last_state == 250)\n",
        "assert(test_agent.last_action == 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1514cb58bcb2499099970c6ef6a11659",
          "grade": false,
          "grade_id": "cell-5e165af2aa84508a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "Xko2pXMmG_Ce"
      },
      "source": [
        "**Expected output**:\n",
        "\n",
        "    Agent state: 250\n",
        "    Agent selected action: 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ef0b7331de522c7a2cf94d65359967b5",
          "grade": false,
          "grade_id": "cell-f6fc34c94aff7aac",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4LaYDd_4G_Ce"
      },
      "source": [
        "Run the following code to test your implementation of the `agent_step()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "76aa2fa9996e9faccf21dbd567b9d808",
          "grade": true,
          "grade_id": "cell-e796695d4d94a2b6",
          "locked": true,
          "points": 40,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5Gcl_I3MG_Ce"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "agent_info = {\n",
        "    \"num_states\": 5,\n",
        "    \"num_hidden_layer\": 1,\n",
        "    \"num_hidden_units\": 2,\n",
        "    \"step_size\": 0.1,\n",
        "    \"discount_factor\": 1.0,\n",
        "    \"beta_m\": 0.9,\n",
        "    \"beta_v\": 0.99,\n",
        "    \"epsilon\": 0.0001,\n",
        "    \"seed\": 0,\n",
        "}\n",
        "\n",
        "test_agent = TDAgent()\n",
        "test_agent.agent_init(agent_info)\n",
        "\n",
        "# load initial weights\n",
        "agent_initial_weight = np.load(\"asserts/agent_step_initial_weights.npz\")\n",
        "test_agent.weights[0][\"W\"] = agent_initial_weight[\"W0\"]\n",
        "test_agent.weights[0][\"b\"] = agent_initial_weight[\"b0\"]\n",
        "test_agent.weights[1][\"W\"] = agent_initial_weight[\"W1\"]\n",
        "test_agent.weights[1][\"b\"] = agent_initial_weight[\"b1\"]\n",
        "\n",
        "# load m and v for the optimizer\n",
        "m_data = np.load(\"asserts/agent_step_initial_m.npz\")\n",
        "test_agent.optimizer.m[0][\"W\"] = m_data[\"W0\"]\n",
        "test_agent.optimizer.m[0][\"b\"] = m_data[\"b0\"]\n",
        "test_agent.optimizer.m[1][\"W\"] = m_data[\"W1\"]\n",
        "test_agent.optimizer.m[1][\"b\"] = m_data[\"b1\"]\n",
        "\n",
        "v_data = np.load(\"asserts/agent_step_initial_v.npz\")\n",
        "test_agent.optimizer.v[0][\"W\"] = v_data[\"W0\"]\n",
        "test_agent.optimizer.v[0][\"b\"] = v_data[\"b0\"]\n",
        "test_agent.optimizer.v[1][\"W\"] = v_data[\"W1\"]\n",
        "test_agent.optimizer.v[1][\"b\"] = v_data[\"b1\"]\n",
        "\n",
        "# Assume the agent started at State 3\n",
        "start_state = 3\n",
        "test_agent.agent_start(start_state)\n",
        "\n",
        "# Assume the reward was 10.0 and the next state observed was State 1\n",
        "reward = 10.0\n",
        "next_state = 1\n",
        "test_agent.agent_step(reward, next_state)\n",
        "\n",
        "agent_updated_weight_answer = np.load(\"asserts/agent_step_updated_weights.npz\")\n",
        "assert(np.allclose(test_agent.weights[0][\"W\"], agent_updated_weight_answer[\"W0\"]))\n",
        "assert(np.allclose(test_agent.weights[0][\"b\"], agent_updated_weight_answer[\"b0\"]))\n",
        "assert(np.allclose(test_agent.weights[1][\"W\"], agent_updated_weight_answer[\"W1\"]))\n",
        "assert(np.allclose(test_agent.weights[1][\"b\"], agent_updated_weight_answer[\"b1\"]))\n",
        "\n",
        "assert(test_agent.last_state == 1)\n",
        "assert(test_agent.last_action == 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5e8f2f2e03598ae43e3c5bf35a72fd4f",
          "grade": false,
          "grade_id": "cell-afbf8dba31f1cd3f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ms1hwFGyG_Cf"
      },
      "source": [
        "**Expected output**:\n",
        "\n",
        "    updated_weights[0][\"W\"]\n",
        "     [[ 1.10893459  0.30763738]\n",
        "     [ 0.63690565  1.14778865]\n",
        "     [ 1.23397791 -0.48152743]\n",
        "     [ 0.72792093 -0.15829832]\n",
        "     [ 0.15021996  0.39822163]]\n",
        "\n",
        "    updated_weights[0][\"b\"]\n",
        "     [[0.29798822 0.96254535]]\n",
        "\n",
        "    updated_weights[1][\"W\"]\n",
        "     [[0.76628754]\n",
        "     [0.11486511]]\n",
        "\n",
        "    updated_weights[1][\"b\"]\n",
        "     [[0.58530057]]\n",
        "\n",
        "    Agent last state: 1\n",
        "    Agent last action: 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "87cd2bb9030402473b202427fef5cfa1",
          "grade": false,
          "grade_id": "cell-42881c5dfc2d4cbb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "lwZBOCjIG_Cf"
      },
      "source": [
        "Run the following code to test your implementation of the `agent_end()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d29d83403f4b90543e14501ae7b263cd",
          "grade": true,
          "grade_id": "cell-484c049db4494d2e",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "KMFbwJ6GG_Cf"
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "# The contents of the cell will be tested by the autograder.\n",
        "# If they do not pass here, they will not pass there.\n",
        "\n",
        "agent_info = {\n",
        "    \"num_states\": 5,\n",
        "    \"num_hidden_layer\": 1,\n",
        "    \"num_hidden_units\": 2,\n",
        "    \"step_size\": 0.1,\n",
        "    \"discount_factor\": 1.0,\n",
        "    \"beta_m\": 0.9,\n",
        "    \"beta_v\": 0.99,\n",
        "    \"epsilon\": 0.0001,\n",
        "    \"seed\": 0,\n",
        "}\n",
        "\n",
        "test_agent = TDAgent()\n",
        "test_agent.agent_init(agent_info)\n",
        "\n",
        "# load initial weights\n",
        "agent_initial_weight = np.load(\"asserts/agent_end_initial_weights.npz\")\n",
        "test_agent.weights[0][\"W\"] = agent_initial_weight[\"W0\"]\n",
        "test_agent.weights[0][\"b\"] = agent_initial_weight[\"b0\"]\n",
        "test_agent.weights[1][\"W\"] = agent_initial_weight[\"W1\"]\n",
        "test_agent.weights[1][\"b\"] = agent_initial_weight[\"b1\"]\n",
        "\n",
        "# load m and v for the optimizer\n",
        "m_data = np.load(\"asserts/agent_step_initial_m.npz\")\n",
        "test_agent.optimizer.m[0][\"W\"] = m_data[\"W0\"]\n",
        "test_agent.optimizer.m[0][\"b\"] = m_data[\"b0\"]\n",
        "test_agent.optimizer.m[1][\"W\"] = m_data[\"W1\"]\n",
        "test_agent.optimizer.m[1][\"b\"] = m_data[\"b1\"]\n",
        "\n",
        "v_data = np.load(\"asserts/agent_step_initial_v.npz\")\n",
        "test_agent.optimizer.v[0][\"W\"] = v_data[\"W0\"]\n",
        "test_agent.optimizer.v[0][\"b\"] = v_data[\"b0\"]\n",
        "test_agent.optimizer.v[1][\"W\"] = v_data[\"W1\"]\n",
        "test_agent.optimizer.v[1][\"b\"] = v_data[\"b1\"]\n",
        "\n",
        "# Assume the agent started at State 4\n",
        "start_state = 4\n",
        "test_agent.agent_start(start_state)\n",
        "\n",
        "# Assume the reward was 10.0 and reached the terminal state\n",
        "reward = 10.0\n",
        "test_agent.agent_end(reward)\n",
        "\n",
        "# updated weights asserts\n",
        "agent_updated_weight_answer = np.load(\"asserts/agent_end_updated_weights.npz\")\n",
        "assert(np.allclose(test_agent.weights[0][\"W\"], agent_updated_weight_answer[\"W0\"]))\n",
        "assert(np.allclose(test_agent.weights[0][\"b\"], agent_updated_weight_answer[\"b0\"]))\n",
        "assert(np.allclose(test_agent.weights[1][\"W\"], agent_updated_weight_answer[\"W1\"]))\n",
        "assert(np.allclose(test_agent.weights[1][\"b\"], agent_updated_weight_answer[\"b1\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d9d2c1b6dcaf0378d7b8834ac978c7a8",
          "grade": false,
          "grade_id": "cell-a5332c5595aecb55",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "YMS0YhhzG_Cf"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "    updated_weights[0][\"W\"]\n",
        "     [[ 1.10893459  0.30763738]\n",
        "     [ 0.63690565  1.14778865]\n",
        "     [ 1.17531054 -0.51043162]\n",
        "     [ 0.75062903 -0.13736817]\n",
        "     [ 0.15021996  0.39822163]]\n",
        "\n",
        "    updated_weights[0][\"b\"]\n",
        "     [[0.30846523 0.95937346]]\n",
        "\n",
        "    updated_weights[1][\"W\"]\n",
        "     [[0.68861703]\n",
        "     [0.15986364]]\n",
        "\n",
        "    updated_weights[1][\"b\"]\n",
        "     [[0.586074]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "adfb54912bfe7b6fb6ca49f261cafcef",
          "grade": false,
          "grade_id": "cell-7df1d6c6d8edab93",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "P02EQhG5G_Cf"
      },
      "source": [
        "## Section 2 - Run Experiment\n",
        "\n",
        "Now that you implemented the agent, we can run the experiment. Similar to Course 3 Programming Assignment 1, we will plot the learned state value function and the learning curve of the TD agent. To plot the learning curve, we use Root Mean Squared Value Error (RMSVE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7864b4c9100769d2fecf4710bcc36a1f",
          "grade": false,
          "grade_id": "cell-6e2643e3d4a46743",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "s1KaJxbAG_Cf"
      },
      "source": [
        "## 2-1: Run Experiment for Semi-gradient TD with a Neural Network\n",
        "\n",
        "We have already provided you the experiment/plot code, so you can go ahead and run the two cells below.\n",
        "\n",
        "Note that running the cell below will take **approximately 12 minutes**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c3b9e5598db38d8c1f2178d0adaa7b86",
          "grade": false,
          "grade_id": "cell-a8a2e1fd49dde097",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "-TIvqpFNG_Cg"
      },
      "outputs": [],
      "source": [
        "# ---------------\n",
        "# Discussion Cell\n",
        "# ---------------\n",
        "\n",
        "true_state_val = np.load('data/true_V.npy')\n",
        "state_distribution = np.load('data/state_distribution.npy')\n",
        "\n",
        "def calc_RMSVE(learned_state_val):\n",
        "    assert(len(true_state_val) == len(learned_state_val) == len(state_distribution))\n",
        "    MSVE = np.sum(np.multiply(state_distribution, np.square(true_state_val - learned_state_val)))\n",
        "    RMSVE = np.sqrt(MSVE)\n",
        "    return RMSVE\n",
        "\n",
        "# Define function to run experiment\n",
        "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
        "\n",
        "    rl_glue = RLGlue(environment, agent)\n",
        "\n",
        "    # save rmsve at the end of each episode\n",
        "    agent_rmsve = np.zeros((experiment_parameters[\"num_runs\"],\n",
        "                            int(experiment_parameters[\"num_episodes\"]/experiment_parameters[\"episode_eval_frequency\"]) + 1))\n",
        "\n",
        "    # save learned state value at the end of each run\n",
        "    agent_state_val = np.zeros((experiment_parameters[\"num_runs\"],\n",
        "                                environment_parameters[\"num_states\"]))\n",
        "\n",
        "    env_info = {\"num_states\": environment_parameters[\"num_states\"],\n",
        "                \"start_state\": environment_parameters[\"start_state\"],\n",
        "                \"left_terminal_state\": environment_parameters[\"left_terminal_state\"],\n",
        "                \"right_terminal_state\": environment_parameters[\"right_terminal_state\"]}\n",
        "\n",
        "    agent_info = {\"num_states\": environment_parameters[\"num_states\"],\n",
        "                  \"num_hidden_layer\": agent_parameters[\"num_hidden_layer\"],\n",
        "                  \"num_hidden_units\": agent_parameters[\"num_hidden_units\"],\n",
        "                  \"step_size\": agent_parameters[\"step_size\"],\n",
        "                  \"discount_factor\": environment_parameters[\"discount_factor\"],\n",
        "                  \"beta_m\": agent_parameters[\"beta_m\"],\n",
        "                  \"beta_v\": agent_parameters[\"beta_v\"],\n",
        "                  \"epsilon\": agent_parameters[\"epsilon\"]\n",
        "                 }\n",
        "\n",
        "    print('Setting - Neural Network with 100 hidden units')\n",
        "    os.system('sleep 1')\n",
        "\n",
        "    # one agent setting\n",
        "    for run in tqdm(range(1, experiment_parameters[\"num_runs\"]+1)):\n",
        "        env_info[\"seed\"] = run\n",
        "        agent_info[\"seed\"] = run\n",
        "        rl_glue.rl_init(agent_info, env_info)\n",
        "\n",
        "        # Compute initial RMSVE before training\n",
        "        current_V = rl_glue.rl_agent_message(\"get state value\")\n",
        "        agent_rmsve[run-1, 0] = calc_RMSVE(current_V)\n",
        "\n",
        "        for episode in range(1, experiment_parameters[\"num_episodes\"]+1):\n",
        "            # run episode\n",
        "            rl_glue.rl_episode(0) # no step limit\n",
        "\n",
        "            if episode % experiment_parameters[\"episode_eval_frequency\"] == 0:\n",
        "                current_V = rl_glue.rl_agent_message(\"get state value\")\n",
        "                agent_rmsve[run-1, int(episode/experiment_parameters[\"episode_eval_frequency\"])] = calc_RMSVE(current_V)\n",
        "            elif episode == experiment_parameters[\"num_episodes\"]: # if last episode\n",
        "                current_V = rl_glue.rl_agent_message(\"get state value\")\n",
        "\n",
        "        agent_state_val[run-1, :] = current_V\n",
        "\n",
        "    save_name = \"{}\".format(rl_glue.agent.name).replace('.','')\n",
        "\n",
        "    if not os.path.exists('results'):\n",
        "                os.makedirs('results')\n",
        "\n",
        "    # save avg. state value\n",
        "    np.save(\"results/V_{}\".format(save_name), agent_state_val)\n",
        "\n",
        "    # save avg. rmsve\n",
        "    np.savez(\"results/RMSVE_{}\".format(save_name), rmsve = agent_rmsve,\n",
        "                                                   eval_freq = experiment_parameters[\"episode_eval_frequency\"],\n",
        "                                                   num_episodes = experiment_parameters[\"num_episodes\"])\n",
        "\n",
        "\n",
        "# Run Experiment\n",
        "\n",
        "# Experiment parameters\n",
        "experiment_parameters = {\n",
        "    \"num_runs\" : 20,\n",
        "    \"num_episodes\" : 1000,\n",
        "    \"episode_eval_frequency\" : 10 # evaluate every 10 episode\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "environment_parameters = {\n",
        "    \"num_states\" : 500,\n",
        "    \"start_state\" : 250,\n",
        "    \"left_terminal_state\" : 0,\n",
        "    \"right_terminal_state\" : 501,\n",
        "    \"discount_factor\" : 1.0\n",
        "}\n",
        "\n",
        "# Agent parameters\n",
        "agent_parameters = {\n",
        "    \"num_hidden_layer\": 1,\n",
        "    \"num_hidden_units\": 100,\n",
        "    \"step_size\": 0.001,\n",
        "    \"beta_m\": 0.9,\n",
        "    \"beta_v\": 0.999,\n",
        "    \"epsilon\": 0.0001,\n",
        "}\n",
        "\n",
        "current_env = RandomWalkEnvironment\n",
        "current_agent = TDAgent\n",
        "\n",
        "# run experiment\n",
        "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n",
        "\n",
        "# plot result\n",
        "plot_script.plot_result([\"td_agent\"])\n",
        "\n",
        "shutil.make_archive('results', 'zip', 'results')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "50d059210779efacbc4a21844fea6c55",
          "grade": false,
          "grade_id": "cell-cca8cda1f7e608b6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "i3ngAD8yG_Cg"
      },
      "source": [
        "You plotted the learning curve for 1000 episodes. As you can see the RMSVE is still decreasing. Here we provide the pre-computed result for 5000 episodes and 20 runs so that you can see the performance of semi-gradient TD with a neural network after being trained for a long time.\n",
        "\n",
        "![](nn_5000_episodes.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e174ad1ddd3b7d140e21e1d9dea8115b",
          "grade": false,
          "grade_id": "cell-5333ab4153a51ac8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0qCUSPGhG_Cg"
      },
      "source": [
        "Does semi-gradient TD with a neural network find a good approximation within 5000 episodes?\n",
        "\n",
        "As you may remember from the previous assignment, semi-gradient TD with 10-state aggregation converged within 100 episodes. Why is TD with a neural network slower?\n",
        "\n",
        "Would it be faster if we decrease the number of hidden units? Or what about if we increase the number of hidden units?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "359a5719ed6b4bb4e6d09d22d432a553",
          "grade": false,
          "grade_id": "cell-055eb25e4e8e690c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "sBEL8f3gG_Cg"
      },
      "source": [
        "## 2-2: Compare Performance of Semi-gradient TD with a Neural Network and Semi-gradient TD with Tile-coding\n",
        "\n",
        "In this section, we compare the performance of semi-gradient TD with a Neural Network and semi-gradient TD with tile-coding. Tile-coding is a kind of coarse coding that uses multiple overlapping partitions of the state space to produce features. For tile-coding, we used 50 tilings each with 6 tiles. We set the step-size for semi-gradient TD with tile-coding to $\\frac{0.1}{\\text{# tilings}}$. See the figure below for the comparison between semi-gradient TD with tile-coding and semi-gradient TD with a neural network and Adam algorithm. This result is for 5000 episodes and 20 runs:\n",
        "![](nn_vs_tc.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3e982fb1b242ff8707a2b451c95b7597",
          "grade": false,
          "grade_id": "cell-5fe17c3095f0843f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "VRv4ogR2G_Cg"
      },
      "source": [
        "How are the results?\n",
        "\n",
        "Semi-gradient TD with tile-coding is much faster than semi-gradient TD with a neural network. Why?\n",
        "\n",
        "Which method has a lower RMSVE at the end of 5000 episodes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "91679a10249eb23e2c9f57013e011ad3",
          "grade": false,
          "grade_id": "cell-1487765e457eca48",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "6xf7r3DOG_Cg"
      },
      "source": [
        "### Wrapping up!\n",
        "\n",
        "You have successfully implemented Course 3 Programming Assignment 2.\n",
        "\n",
        "You have implemented **semi-gradient TD with a Neural Network and Adam algorithm** in 500-state Random Walk.\n",
        "\n",
        "You also compared semi-gradient TD with a neural network and semi-gradient TD with tile-coding.\n",
        "\n",
        "From the experiments and lectures, you should be more familiar with some of the strengths and weaknesses of using neural networks as the function approximator for an RL agent. On one hand, neural networks are powerful function approximators capable of representing a wide class of functions. They are also capable of producing features without exclusively relying on hand-crafted mechanisms. On the other hand, compared to a linear function approximator with tile-coding, neural networks can be less sample efficient. When implementing your own Reinforcement Learning agents, you may consider these strengths and weaknesses to choose the proper function approximator for your problems."
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "prediction-control-function-approximation",
      "graded_item_id": "ZJrJN",
      "launcher_item_id": "jSYQa"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
